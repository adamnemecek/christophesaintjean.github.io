{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python3.5/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import site, os\n",
    "try:\n",
    "    print(site.getsitepackages())\n",
    "except:\n",
    "    print(os.path.dirname(site.__file__) + '/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as sci\n",
    "import scipy.optimize as sco\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "%matplotlib qt4\n",
    "#print (matplotlib.rcsetup.interactive_bk)\n",
    "#print (matplotlib.rcsetup.non_interactive_bk)\n",
    "#print (matplotlib.rcsetup.all_backends)\n",
    "print (matplotlib.get_backend())\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for gaussian distribution as an exponential family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s_1(x):\n",
    "    return x\n",
    "\n",
    "def s_2(x):\n",
    "    return -x*x\n",
    "\n",
    "def F_1D(theta1,theta2):\n",
    "    return 0.25*theta1*theta1/theta2 + 0.5*np.log(np.pi) - 0.5*np.log(theta2) \n",
    "\n",
    "def gradF_1_1D(theta_1,theta_2):\n",
    "    return 0.5*theta_1/theta_2\n",
    "\n",
    "def gradF_2_1D(theta_1, theta_2):\n",
    "    temp_1 = 0.5 / theta_2\n",
    "    temp_2 = temp_1 * theta_1\n",
    "    return -1. * (temp_2 * temp_2 + temp_1)\n",
    "\n",
    "def gradF_1_nD(theta_1,theta_2):\n",
    "    return 0.5*np.dot(np.inv(theta_2), theta_1)\n",
    "\n",
    "def gradF_2_nD(theta_1, theta_2):\n",
    "    temp_1 = 0.5*np.inv(theta_2)\n",
    "    temp_2 = np.dot(temp1,theta_1)\n",
    "    return - np.outer(temp_2,temp_2) - temp_1\n",
    "\n",
    "def gradG_1_1D(eta_1,eta_2):\n",
    "    return eta1 / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_2_1D(eta_1,eta_2):\n",
    "    return 0.5  / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_1_nD(eta_1,eta_2):\n",
    "    return np.dot(np.inv(-np.outer(eta_1, eta1) - eta2), eta1)\n",
    "\n",
    "def gradG_2_nD(eta_1,eta_2):\n",
    "    return 0.5*np.inv(-np.outer(eta_1, eta1) - eta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas 1D - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "N, batch_size = 2000, 100\n",
    "mu_true, sigma_true = 1, 2\n",
    "sigma2_true = sigma_true*sigma_true\n",
    "theta1_true = mu_true / sigma2_true\n",
    "theta2_true = 0.5 / sigma2_true\n",
    "eta1_true = mu_true\n",
    "eta2_true = - (mu_true*mu_true + sigma2_true)\n",
    "\n",
    "X = np.random.normal(mu_true,np.sqrt(sigma2_true), N)\n",
    "\n",
    "def batches(Y, bs):\n",
    "    nb = np.int(np.ceil(len(Y) / bs))\n",
    "    Yb = [Y[i * bs : (i+1) * bs] for i in range(nb)]\n",
    "    def __temp(i):\n",
    "        return Yb[i]\n",
    "    return nb, Yb, __temp\n",
    "\n",
    "Xt = X[: N//4]\n",
    "Nt = len(Xt)\n",
    "NtB, XtB, XtB_f = batches(Xt, batch_size)\n",
    "Xv = X[N//4 : N//2]\n",
    "Nv = len(Xv)\n",
    "NvB, XvB, XvB_f = batches(Xv, batch_size)\n",
    "Xtest = X[N//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(x, mu, sigma2):\n",
    "    return -(x - mu)**2 /(2 * sigma2) - np.log(np.sqrt(2 * sigma2 * np.pi)) \n",
    "\n",
    "def ll_theta(x, theta1, theta2):\n",
    "    return s_1(x) * theta1 + s_2(x) * theta2 - F_1D(theta1, theta2)\n",
    "\n",
    "def ll_eta(x, eta1, eta2):\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    return ll_theta(x, theta1, theta2)\n",
    "    \n",
    "\n",
    "def ave_ll(mu, sigma2, chi):\n",
    "    N = len(chi)\n",
    "    return (1. / N) * sum(ll(x, mu, sigma2) for x in chi)\n",
    "    \n",
    "def C_N(mu, sigma2, chi):\n",
    "    return -ave_ll(mu, sigma2, chi)\n",
    "\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_theta(4, theta1_true, theta2_true))\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_eta(4, eta1_true, eta2_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "def disp_results(results):\n",
    "    n = len(results)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n))\n",
    "    table = ListTable()\n",
    "    table.append(['', 'Train', 'Test', 'mu', 'sigma^2'])\n",
    "    table.append(['True', ave_ll(mu_true, sigma2_true, Xt), \n",
    "                  ave_ll(mu_true, sigma2_true, Xtest),\n",
    "                  str(mu_true), str(sigma_true**2)])\n",
    "    for c, result in enumerate(results):\n",
    "        train, test, mu, sig, method = result\n",
    "        axes[0,0].plot(train, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,0].set_title('Average likelihood on train set')\n",
    "        axes[0,0].legend(loc=4)\n",
    "        axes[0,1].plot(test, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,1].set_title('Average likelihood on test set')\n",
    "        axes[0,1].legend(loc=4)\n",
    "        axes[1,0].plot(mu, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,0].set_title('Estimates of $\\mu$')\n",
    "        axes[1,0].legend(loc=4)\n",
    "        axes[1,1].plot(sig, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,1].set_title('Estimates of $\\sigma^2$')\n",
    "        axes[1,1].legend(loc=4)\n",
    "        table.append([method, train[-1], test[-1],mu[-1], sig[-1]])\n",
    "    display(HTML(table._repr_html_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem = joblib.Memory(cachedir='/tmp/joblib')\n",
    "\n",
    "import Parallel, delayed\n",
    ">>> from math import sqrt\n",
    ">>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
    "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "def result_results(fun, **kwargs):\n",
    "    \n",
    "    mem = Memory(cachedir='/tmp/joblib')\n",
    "    \n",
    "\n",
    "from joblib import Memory\n",
    ">>> mem = Memory(cachedir='/tmp/joblib')\n",
    ">>> import numpy as np\n",
    ">>> a = np.vander(np.arange(3)).astype(np.float)\n",
    ">>> square = mem.cache(np.square)\n",
    ">>> b = square(a)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points stationnaires de $C(\\theta) = \\mathbb{E}_{\\pi} [C(\\theta,x)] = \\mathbb{E}_{\\pi} [-\\log p(x;\\theta)]$ par Robbins-Monro\n",
    "\n",
    "\n",
    "### Robbins-Monro\n",
    "\n",
    "On dispose d'une fonction inconnue (supposée monotone) $M(\\theta)$ telle que \n",
    "$$M(\\theta) = \\mathbb{E}_{\\pi(\\beta|\\theta)} [\\beta]$$ \n",
    "avec $\\beta$ une v.a désignant des observations bruitées de $M(\\theta)$.\n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\alpha$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} - \\theta^{(t)} = a^{(t)} (\\alpha - \\beta^{(t)})$$\n",
    "\n",
    "### Robbins-Monro  et gradient stochastique\n",
    "\n",
    "$M(\\theta) := \\nabla_{\\theta} C(\\theta)$ est le gradient d'une fonction inconnue $C$. \n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\nabla C(\\theta^*) = 0$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} = \\theta^{(t)} - a^{(t)} \\beta^{(t)}$$\n",
    "où $\\beta^{(t)}$ est une observation bruitée de $\\nabla_{\\theta} C(\\theta^{(t)})$.\n",
    "\n",
    "### Pour notre cas\n",
    "La fonction à minimiser est:\n",
    "$$C(\\theta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\theta)]$$\n",
    "où $\\pi$ est la distribution inconnue dont on cherche une approximation $p$ paramétrée par $\\theta$ (identique à la minimisation sur $\\theta$ de $KL(\\pi || p(.;\\theta))$)\n",
    "\n",
    "Son équivalent en discret:\n",
    "$$C_N(\\theta) = - N^{-1} \\sum_i \\log p(x_i;\\theta)$$\n",
    "avec $\\lim_{N \\rightarrow +\\infty} C_N(\\theta) = C(\\theta)$ \n",
    "\n",
    "Sous conditions de regularité et dans la famille exponentielle, \n",
    "$$\\nabla_{\\theta} C(\\theta) = \\mathbb{E}_{\\pi} [- \\nabla_{\\theta}\\log p(x;\\theta)]  = \\mathbb{E}_{\\pi} [- s(x) + \\nabla_{\\theta} F (\\theta)]$$\n",
    "\n",
    "Pour le relier à Robbin-Monro, on a une observation bruitée\n",
    "$$\\beta^{(t)} = - s(x^{(t)}) + \\nabla_{\\theta} F (\\theta^{(t)})$$\n",
    "et donc la suite convergente $$\\theta^{(t+1)} = \\theta^{(t)} + a^{(t)} (s(x^{(t)}) - \\nabla_{\\theta} F (\\theta^{(t)}))$$\n",
    "\n",
    "Est ce que la formulation suivante est équivalente ? sans doute non ...\n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} (s(x^{(t)}) - \\eta^{(t)}))$$\n",
    "\n",
    "Dans l'espace $H$ (paramètre d'espérance), la même optimisation:\n",
    "$$C(\\eta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\eta)] = \\mathbb{E}_{\\pi} [B_{F^*}(s(x) : \\eta) - F^*(s(x)) - k(x)]$$\n",
    "$$\\nabla_{\\eta} C(\\eta) = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} B_{F^*}(s(x) : \\eta)] = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} (F^*(s(x)) - F^*(\\eta) - <s(x) - \\eta, \\nabla_\\eta F^*(\\eta)>)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) - [-1,0;0,-1]*\\nabla_\\eta F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) + \\nabla_{\\eta} F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)] = \\mathbb{E}_{\\pi} [- Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "et donc la suite convergente \n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} Hess F^*(\\eta^{(t)}) (s(x^{(t)}) - \\eta^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-(-t + s2(x, y))*Derivative(F(z, t), t, z) - (-z + s1(x, y))*Derivative(F(z, t), z, z) + Derivative(F(z, t), z)\n",
      "-(-t + s2(x, y))*Derivative(F(z, t), t, t) - (-z + s1(x, y))*Derivative(F(z, t), t, z) + Derivative(F(z, t), t)\n",
      "8.0*eta1**2/(-2*eta1**2 - 2*eta2)**2 + 2.0/(-2*eta1**2 - 2*eta2)\n",
      "4.0*eta1/(-2*eta1**2 - 2*eta2)**2\n",
      "4.0*eta1/(-2*eta1**2 - 2*eta2)**2\n",
      "2.0/(-2*eta1**2 - 2*eta2)**2\n",
      "(1.0*eta1*(eta2 + x**2) + 1.0*(eta1 - x)*(eta1**2 - eta2))/(eta1**2 + eta2)**2\n",
      "(1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n"
     ]
    }
   ],
   "source": [
    "from sympy import Function, Derivative, var, simplify\n",
    "from sympy.abc import x, y, z, t\n",
    "from sympy import diff, log, pi\n",
    "s1 = Function(\"s1\")(x,y)\n",
    "s2 = Function(\"s2\")(x,y)\n",
    "F = Function(\"F\")(z,t)\n",
    "F1 = Derivative(F, z)\n",
    "F2 = Derivative(F, t)\n",
    "expr = -((s1 - z) * F1 + (s2 - t)*F2)\n",
    "print (diff(expr, z))\n",
    "print (diff(expr, t))\n",
    "x, eta1, eta2 = var('x eta1 eta2')\n",
    "s1 = x\n",
    "s2 = -x*x\n",
    "ld = log(2*(- eta1*eta1 - eta2))\n",
    "F =  - 0.5 * (1 + log(pi) + ld)\n",
    "dF1 = diff(F, eta1)\n",
    "dF2 = diff(F, eta2)\n",
    "dF11 = diff(dF1, eta1)\n",
    "dF12 = diff(dF1, eta2)\n",
    "dF21 = diff(dF2, eta1)\n",
    "dF22 = diff(dF2, eta2)\n",
    "print (dF11)\n",
    "print (dF12)\n",
    "print (dF21)\n",
    "print (dF22)\n",
    "print (-simplify(dF11*(s1-eta1)+dF12*(s2-eta2)))\n",
    "print (-simplify(dF21*(s1-eta1)+dF22*(s2-eta2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debut des manips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points stationnaires de $C_N(\\theta)$ par dérivation exacte\n",
    "\n",
    "   $$\\nabla C_N(\\theta) = 0 \\equiv -N^{-1} \\sum_i (s(x_i) - \\nabla F(\\theta)) = 0 \\equiv  \\nabla F(\\theta) = N^{-1} \\sum_i s(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradF_pt_stat_1 = np.sum(s_1(x) for x in Xt) / Nt\n",
    "gradF_pt_stat_2 = np.sum(s_2(x) for x in Xt) / Nt\n",
    "pt_stat = gradF_pt_stat_1, - gradF_pt_stat_1**2 - gradF_pt_stat_2\n",
    "print ((mu_true, sigma2_true), ' vs ', pt_stat)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xtest))\n",
    "gradF_pt_stat_1 = np.cumsum([s_1(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "gradF_pt_stat_2 = np.cumsum([s_2(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "mu_list = gradF_pt_stat_1\n",
    "sig_list = -gradF_pt_stat_1**2 -gradF_pt_stat_2\n",
    "ave_ll_train = [ave_ll(mu, sig, Xt) for mu,sig in zip(mu_list, sig_list)]\n",
    "ave_ll_test = [ave_ll(mu, sig, Xtest) for mu,sig in zip(mu_list, sig_list)]\n",
    "this_result = [ave_ll_train, ave_ll_test, mu_list, sig_list, 'Exact']\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results([this_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 2.0495816705453467\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ -1.68753900e-06,  -6.75015599e-06])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 39\n",
      "      nit: 11\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.93141088,  3.52978902])\n",
      "average ll on training set :  -2.0539185454  vs  -2.04958167055\n",
      "average ll on test set :  -2.10152922778  vs  -2.10480108284\n"
     ]
    }
   ],
   "source": [
    "def fun_C_N(mu_sigma2):\n",
    "    return C_N(mu_sigma2[0], mu_sigma2[1], Xt)\n",
    "np.random.rand(seed)\n",
    "x0 = (np.random.randn(), np.random.random())\n",
    "bnds = ((-np.inf, np.inf), (1e-6, np.inf))   # variance is positive\n",
    "res_C_N = sco.minimize(fun_C_N, x0, bounds=bnds) #, options={'gtol': 1e-6, 'disp': True})\n",
    "print(res_C_N)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (natural space)\n",
    "mostly fail:\n",
    "- $\\alpha = 0.01$\n",
    "\n",
    "success:\n",
    "- $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "theta1, theta2 = mu_0 / sigma2_0, 0.5 / sigma2_0\n",
    "\n",
    "def grad_nll_1(x, theta1, theta2):\n",
    "    return - (s_1(x) - gradF_1_1D(theta1, theta2))\n",
    "\n",
    "def grad_nll_2(x, theta1, theta2):\n",
    "    return - (s_2(x) - gradF_2_1D(theta1, theta2))\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for xt in Xt:\n",
    "        #print(\"avant theta\",theta1, theta2, \" x \", xt)\n",
    "        #print (\"grad 1 \", gradF_1_1D(theta1, theta2), grad_nll_1(xt, theta1, theta2))\n",
    "        #print (\"grad 2 \", gradF_2_1D(theta1, theta2), grad_nll_2(xt, theta1, theta2))\n",
    "        theta1 -= alpha*grad_nll_1(xt, theta1, theta2)\n",
    "        theta2 -= alpha*grad_nll_2(xt, theta1, theta2)\n",
    "        #print(\"apres theta\", theta1, theta2)\n",
    "        mu_est, sigma2_est = 0.5*theta1/theta2, 0.5/theta2\n",
    "        #print(\"apres mu_sd2\",   mu_est, sigma2_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(T)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  \n",
    "\n",
    "Il s'agit d'un réécriture simple en remplacant $\\nabla F$ par $\\eta$. \n",
    "\n",
    "Cela ressemble dans la forme à l'approximation stochastique du Online EM.\n",
    "\n",
    "L'algorihme converge car les deux optimisations (avec celles du dessus) sont liées. \n",
    "\n",
    "(J'ai fait le calcul $\\eta^{(n+1)} - \\eta^{(n)}$ vers $\\theta^{(n+1)} - \\theta^{(n)}$)\n",
    "\n",
    "Il ne correspond pas à un SGD dand $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_nll_1(x, eta1, eta2):\n",
    "    return -(s_1(x) - eta1)\n",
    "\n",
    "def grad_nll_2(x, eta1, eta2):\n",
    "    return -(s_2(x) - eta2)\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_nll_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_nll_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(M)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Train</td><td>Test</td><td>mu</td><td>sigma^2</td></tr><tr><td>True</td><td>-2.0539185454014794</td><td>-2.101529227781853</td><td>1</td><td>4</td></tr><tr><td>SGD(T)-0.001</td><td>-2.07042166281291</td><td>-2.1441114500583507</td><td>0.7238246892043892</td><td>2.828639631552067</td></tr><tr><td>SGD(T)-0.001</td><td>-2.070463312248271</td><td>-2.144173698159312</td><td>0.7236743350269396</td><td>2.8279820960377546</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (Expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_1(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta2 + x**2) + (eta1 - x)*(8.0*eta1**2 - 8.0*eta2)/8.)/(eta1**2 + eta2)**2\n",
    "\n",
    "def grad_2(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(H)-CSJ'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Train</td><td>Test</td><td>mu</td><td>sigma^2</td></tr><tr><td>True</td><td>-2.0539185454014794</td><td>-2.101529227781853</td><td>1</td><td>4</td></tr><tr><td>SGD(H)- Theano 0.001</td><td>-2.127535661677559</td><td>-2.220700949893082</td><td>0.5444393472775878</td><td>2.362821014205942</td></tr><tr><td>SGD(H)-CSJ0.001</td><td>-2.1278333040960002</td><td>-2.221087705468255</td><td>0.5440596567516236</td><td>2.3608284762100604</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_updates(cost, params, alpha):\n",
    "    updates = [(param, param - alpha*T.grad(cost, param)) for param in params]\n",
    "    return updates\n",
    "\n",
    "def gradient_updates_momentum(cost, params, alpha, momentum):\n",
    "    \"\"\"\n",
    "    http://caffe.berkeleyvision.org/tutorial/solver.html\n",
    "    \"\"\"\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    updates = []\n",
    "    for param in params:\n",
    "        V = theano.shared(param.get_value()*0.)\n",
    "        updates.append((param, param + V))\n",
    "        updates.append((V, momentum*V - alpha * T.grad(cost, param)))\n",
    "    return updates\n",
    "\n",
    "# on peut rajouter d'autres méthodes ici\n",
    "\n",
    "def gradient_updates_adam(cost, params, alpha, beta1, beta2):\n",
    "    assert beta1 < 1 and beta1 >= 0\n",
    "    assert beta2 < 1 and beta2 >= 0\n",
    "    updates = []\n",
    "    t = theano.shared(1.)\n",
    "    updates.append((t, t+1))\n",
    "    for param in params:\n",
    "        gt = T.grad(cost, param) \n",
    "        mt = theano.shared(0.)\n",
    "        updates.append((mt, beta1 * mt + (1-beta1) * gt))\n",
    "        vt = theano.shared(0.)\n",
    "        updates.append((vt, beta2 * vt + (1-beta2) * gt * gt))\n",
    "        alpha_t = theano.shared(0.)\n",
    "        updates.append((alpha_t, \n",
    "                        alpha*T.sqrt(1 - beta2**t)/(1 - beta1**t)))\n",
    "        updates.append((param, param - alpha_t*mt/(T.sqrt(vt + 1e-8))))\n",
    "    return updates   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization SGD via Theano (source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll, [mu, sigma2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        __ = train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(S)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization SGD via Theano (natural space)\n",
    "\n",
    "Fails for $\\alpha =0.005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random() \n",
    "theta1 = theano.shared(mu_0 / sigma2_0, name=\"theta1\") # ensure initial value is positive\n",
    "theta2 = theano.shared(0.5 / sigma2_0, name=\"theta2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll_theta =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll_theta, [theta1, theta2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est = 0.5*theta1.get_value()/theta2.get_value()\n",
    "        sigma2_est = 0.5/theta2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(T)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization SGD via Theano (expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.712390662050588 -1.3317789880382436\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1 = theano.shared(mu_0, name=\"eta1\") # ensure initial value is positive\n",
    "eta2 = theano.shared(-(mu_0*mu_0 + sigma2_0), name=\"eta2\") # ensure initial value is positive\n",
    "print(eta1.get_value(), eta2.get_value())\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "nll_eta =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll_eta, [eta1, eta2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        # print(g1, s_1(xt) - eta1.get_value())\n",
    "        # print(g2, s_2(xt) - eta2.get_value())\n",
    "        mu_est = eta1.get_value()\n",
    "        sigma2_est = - (eta1.get_value()*eta1.get_value()+eta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(H)- Theano '+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization via Theano +  SGD Nesterov momemtum (Source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates_momentum(nll, [mu, sigma2], alpha, momentum))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'Mom. SGD(S)-'+str(momentum)+','+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization via Theano +  ADAM (Source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.005\n",
    "beta1 = 0.9\n",
    "beta2 = 0.995\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates_adam(nll, [mu, sigma2], \n",
    "                                        alpha, beta1, beta2))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'Adam(S)-'+str(beta1)+','+str(beta2)+','+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via downhill (adam)\n",
    "\n",
    "Pas encore terminé..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "climate.enable_default_logging()\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.vector()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "#nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "navell =  T.sum(T.sqr(x - mu) /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi)))\n",
    "train = downhill.Dataset(Xt)\n",
    "#valid = downhill.Dataset(Xtest)\n",
    "\n",
    "downhill.minimize(\n",
    "    loss=navell,\n",
    "    train=Xt,\n",
    "    patience=0,\n",
    "    max_gradient_norm=1,          # Prevent gradient explosion!\n",
    "    learning_rate=alpha)\n",
    "\n",
    "\n",
    "print(mu.get_value())\n",
    "print(sigma2.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(downhill.minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'exemple de la doc de downhill ne marche pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "climate.enable_default_logging()\n",
    "\n",
    "\n",
    "import theano.tensor as TT\n",
    "def rand(a, b): return np.random.randn(a, b).astype('f')\n",
    "\n",
    "A, B, K = 20, 5, 3\n",
    "\n",
    "# Set up a matrix factorization problem to optimize.\n",
    "u = theano.shared(rand(A, K), name='u')\n",
    "v = theano.shared(rand(K, B), name='v')\n",
    "e = TT.sqr(TT.matrix() - TT.dot(u, v))\n",
    "\n",
    "# Minimize the regularized loss with respect to a data matrix.\n",
    "y = np.dot(rand(A, K), rand(K, B)) + rand(A, B)\n",
    "\n",
    "downhill.minimize(\n",
    "    loss=e.mean() + abs(u).mean() + (v * v).mean(),\n",
    "    train=[y],\n",
    "    patience=0,\n",
    "    batch_size=A,                 # Process y as a single batch.\n",
    "    max_gradient_norm=1,          # Prevent gradient explosion!\n",
    "    learning_rate=0.1,\n",
    "    monitors=(('err', e.mean()),  # Monitor during optimization.\n",
    "              ('|u|<0.1', (abs(u) < 0.1).mean()),\n",
    "              ('|v|<0.1', (abs(v) < 0.1).mean())),\n",
    "    monitor_gradients=True)\n",
    "\n",
    "# Print out the optimized coefficients u and basis v.\n",
    "print('u =', u.get_value())\n",
    "print('v =', v.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
