{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import site, os\n",
    "try:\n",
    "    print(site.getsitepackages())\n",
    "except:\n",
    "    print(os.path.dirname(site.__file__) + '/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "%matplotlib qt4\n",
    "#print (matplotlib.rcsetup.interactive_bk)\n",
    "#print (matplotlib.rcsetup.non_interactive_bk)\n",
    "#print (matplotlib.rcsetup.all_backends)\n",
    "print (matplotlib.get_backend())\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from joblib import Memory, Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for gaussian distribution as an exponential family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lb2th(mu,sigma2):\n",
    "    return mu / sigma2, 0.5 / sigma2\n",
    "\n",
    "def lb2et(mu, sigma2):\n",
    "    return mu, -(mu**2+sigma2)\n",
    "\n",
    "def th2lb(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5 / th2\n",
    "\n",
    "def th2et(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5*(-0.5*th1**2/th2 - 1) / th2\n",
    "\n",
    "def et2lb(et1,et2):\n",
    "    return et1, -(et1**2+et2)\n",
    "\n",
    "def et2th(et1,et2):\n",
    "    return - et1 /(et1**2 + et2), -0.5 /(et1**2 + et2)\n",
    "\n",
    "def s_1(x):\n",
    "    return x\n",
    "\n",
    "def s_2(x):\n",
    "    return -x*x\n",
    "\n",
    "def F_1D(theta1,theta2):\n",
    "    return 0.25*theta1*theta1/theta2 + 0.5*np.log(np.pi) - 0.5*np.log(theta2) \n",
    "\n",
    "def gradF_1_1D(theta_1,theta_2):\n",
    "    return 0.5*theta_1/theta_2\n",
    "\n",
    "def gradF_2_1D(theta_1, theta_2):\n",
    "    temp_1 = 0.5 / theta_2\n",
    "    temp_2 = temp_1 * theta_1\n",
    "    return -1. * (temp_2 * temp_2 + temp_1)\n",
    "\n",
    "def gradF_1_nD(theta_1,theta_2):\n",
    "    return 0.5*np.dot(np.inv(theta_2), theta_1)\n",
    "\n",
    "def gradF_2_nD(theta_1, theta_2):\n",
    "    temp_1 = 0.5*np.inv(theta_2)\n",
    "    temp_2 = np.dot(temp1,theta_1)\n",
    "    return - np.outer(temp_2,temp_2) - temp_1\n",
    "\n",
    "def gradG_1_1D(eta_1,eta_2):\n",
    "    return eta1 / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_2_1D(eta_1,eta_2):\n",
    "    return 0.5  / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_1_nD(eta_1,eta_2):\n",
    "    return np.dot(np.inv(-np.outer(eta_1, eta1) - eta2), eta1)\n",
    "\n",
    "def gradG_2_nD(eta_1,eta_2):\n",
    "    return 0.5*np.inv(-np.outer(eta_1, eta1) - eta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas 1D - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "N, batch_size = 1000*4, 100\n",
    "mu_true, sigma_true = 1, 2\n",
    "sigma2_true = sigma_true**2\n",
    "theta1_true, theta2_true = lb2th(mu_true, sigma2_true)\n",
    "eta1_true, eta2_true = lb2et(mu_true, sigma2_true)\n",
    "X = np.random.normal(mu_true,np.sqrt(sigma2_true), N)\n",
    "replicate = 5\n",
    "mu_0, sigma2_0 = 0, 10\n",
    "a_0 = 1 \n",
    "lb_init = [(mu, sigma2) for mu, sigma2 in \n",
    "           zip(np.random.normal(mu_0,np.sqrt(sigma2_0), replicate),\n",
    "               scs.invgamma.rvs(a_0, size=replicate))]\n",
    "th_init = [lb2th(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "et_init = [lb2et(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "#print(lb_init, th_init, et_init)\n",
    "def batches(Y, bs):\n",
    "    nb = np.int(np.ceil(len(Y) / bs))\n",
    "    Yb = [Y[i * bs : (i+1) * bs] for i in range(nb)]\n",
    "    def __temp(i):\n",
    "        return Yb[i]\n",
    "    return nb, Yb, __temp\n",
    "\n",
    "Xt = X[: N//4]\n",
    "Nt = len(Xt)\n",
    "NtB, XtB, XtB_f = batches(Xt, batch_size)\n",
    "Xv = X[N//4 : N//2]\n",
    "Nv = len(Xv)\n",
    "NvB, XvB, XvB_f = batches(Xv, batch_size)\n",
    "Xtest = X[N//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(x, mu, sigma2):\n",
    "    return -(x - mu)**2 /(2 * sigma2) - np.log(np.sqrt(2 * sigma2 * np.pi)) \n",
    "\n",
    "def ll_theta(x, theta1, theta2):\n",
    "    return s_1(x) * theta1 + s_2(x) * theta2 - F_1D(theta1, theta2)\n",
    "\n",
    "def ll_eta(x, eta1, eta2):\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    return ll_theta(x, theta1, theta2)\n",
    "\n",
    "def kl(mu_1, s_1, mu_2, s_2):\n",
    "    return 0.5*(s_1/s_2 - np.log(s_1/s_2) - (mu_1 - mu_2)**2 / s_2  - 1)\n",
    "    \n",
    "def ave_ll(mu, sigma2, chi):\n",
    "    N = len(chi)\n",
    "    return (1. / N) * sum(ll(x, mu, sigma2) for x in chi)\n",
    "    \n",
    "def C_N(mu, sigma2, chi):\n",
    "    return -ave_ll(mu, sigma2, chi)\n",
    "\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_theta(4, theta1_true, theta2_true))\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_eta(4, eta1_true, eta2_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "\n",
    "def customize_boxplot(ax, bp):\n",
    "    for box in bp['boxes']:\n",
    "        # change outline color\n",
    "        box.set( color='#7570b3', linewidth=2)\n",
    "        # change fill color\n",
    "        box.set( facecolor = '#1b9e77' )\n",
    "    ## change color and linewidth of the whiskers\n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color='#7570b3', linewidth=2)\n",
    "    ## change color and linewidth of the caps\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color='#7570b3', linewidth=2)\n",
    "    ## change color and linewidth of the medians\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='#b2df8a', linewidth=2)\n",
    "    ## change the style of fliers and their fill\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "    ## Custom x-axis labels\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    return ax, bp\n",
    "    \n",
    "def ll_boxplot(list_mu_sig_list, X, ticks):\n",
    "    data_to_plot = []\n",
    "    labels = [str(n) for n in ticks]\n",
    "    for n in ticks:\n",
    "        data_n = [ave_ll(mu_list[n-1], sig_list[n-1], X) \n",
    "                  for mu_list, sig_list in list_mu_sig_list]\n",
    "        data_to_plot.append(data_n)    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax, bp = customize_boxplot(ax, bp)\n",
    "    \n",
    "def kl_boxplot(list_mu_sig_list, X, ticks):\n",
    "    data_to_plot = []\n",
    "    labels = [str(n) for n in ticks]\n",
    "    for n in ticks:\n",
    "        data_n = [kl(mu_list[n-1], sig_list[n-1], mu_true, sigma2_true) \n",
    "                  for mu_list, sig_list in list_mu_sig_list]\n",
    "        data_to_plot.append(data_n)    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax, bp = customize_boxplot(ax, bp)\n",
    "    \n",
    "\n",
    "def disp_results_all(results):\n",
    "    n = len(results)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n))\n",
    "    table = ListTable()\n",
    "    table.append(['', 'Train', 'Test', 'mu', 'sigma^2'])\n",
    "    table.append(['True', ave_ll(mu_true, sigma2_true, Xt), \n",
    "                  ave_ll(mu_true, sigma2_true, Xtest),\n",
    "                  str(mu_true), str(sigma_true**2)])\n",
    "    for c, result in enumerate(results):\n",
    "        train, test, mu, sig, method = result\n",
    "        axes[0,0].plot(train, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,0].set_title('Average likelihood on train set')\n",
    "        axes[0,0].legend(loc=4)\n",
    "        axes[0,1].plot(test, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,1].set_title('Average likelihood on test set')\n",
    "        axes[0,1].legend(loc=4)\n",
    "        axes[1,0].plot(mu, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,0].set_title('Estimates of $\\mu$')\n",
    "        axes[1,0].legend(loc=4)\n",
    "        axes[1,1].plot(sig, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,1].set_title('Estimates of $\\sigma^2$')\n",
    "        axes[1,1].legend(loc=4)\n",
    "        table.append([method, train[-1], test[-1],mu[-1], sig[-1]])\n",
    "    display(HTML(table._repr_html_()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points stationnaires de $C(\\theta) = \\mathbb{E}_{\\pi} [C(\\theta,x)] = \\mathbb{E}_{\\pi} [-\\log p(x;\\theta)]$ par Robbins-Monro\n",
    "\n",
    "\n",
    "### Robbins-Monro\n",
    "\n",
    "On dispose d'une fonction inconnue (supposée monotone) $M(\\theta)$ telle que \n",
    "$$M(\\theta) = \\mathbb{E}_{\\pi(\\beta|\\theta)} [\\beta]$$ \n",
    "avec $\\beta$ une v.a désignant des observations bruitées de $M(\\theta)$.\n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\alpha$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} - \\theta^{(t)} = a^{(t)} (\\alpha - \\beta^{(t)})$$\n",
    "\n",
    "### Robbins-Monro  et gradient stochastique\n",
    "\n",
    "$M(\\theta) := \\nabla_{\\theta} C(\\theta)$ est le gradient d'une fonction inconnue $C$. \n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\nabla C(\\theta^*) = 0$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} = \\theta^{(t)} - a^{(t)} \\beta^{(t)}$$\n",
    "où $\\beta^{(t)}$ est une observation bruitée de $\\nabla_{\\theta} C(\\theta^{(t)})$.\n",
    "\n",
    "### Pour notre cas\n",
    "La fonction à minimiser est:\n",
    "$$C(\\theta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\theta)]$$\n",
    "où $\\pi$ est la distribution inconnue dont on cherche une approximation $p$ paramétrée par $\\theta$ (identique à la minimisation sur $\\theta$ de $KL(\\pi || p(.;\\theta))$)\n",
    "\n",
    "Son équivalent en discret:\n",
    "$$C_N(\\theta) = - N^{-1} \\sum_i \\log p(x_i;\\theta)$$\n",
    "avec $\\lim_{N \\rightarrow +\\infty} C_N(\\theta) = C(\\theta)$ \n",
    "\n",
    "Sous conditions de regularité et dans la famille exponentielle, \n",
    "$$\\nabla_{\\theta} C(\\theta) = \\mathbb{E}_{\\pi} [- \\nabla_{\\theta}\\log p(x;\\theta)]  = \\mathbb{E}_{\\pi} [- s(x) + \\nabla_{\\theta} F (\\theta)]$$\n",
    "\n",
    "Pour le relier à Robbin-Monro, on a une observation bruitée\n",
    "$$\\beta^{(t)} = - s(x^{(t)}) + \\nabla_{\\theta} F (\\theta^{(t)})$$\n",
    "et donc la suite convergente $$\\theta^{(t+1)} = \\theta^{(t)} + a^{(t)} (s(x^{(t)}) - \\nabla_{\\theta} F (\\theta^{(t)}))$$\n",
    "\n",
    "Est ce que la formulation suivante est équivalente ? sans doute non ...\n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} (s(x^{(t)}) - \\eta^{(t)}))$$\n",
    "\n",
    "Dans l'espace $H$ (paramètre d'espérance), la même optimisation:\n",
    "$$C(\\eta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\eta)] = \\mathbb{E}_{\\pi} [B_{F^*}(s(x) : \\eta) - F^*(s(x)) - k(x)]$$\n",
    "$$\\nabla_{\\eta} C(\\eta) = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} B_{F^*}(s(x) : \\eta)] = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} (F^*(s(x)) - F^*(\\eta) - <s(x) - \\eta, \\nabla_\\eta F^*(\\eta)>)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) - [-1,0;0,-1]*\\nabla_\\eta F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) + \\nabla_{\\eta} F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)] = \\mathbb{E}_{\\pi} [- Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "et donc la suite convergente \n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} Hess F^*(\\eta^{(t)}) (s(x^{(t)}) - \\eta^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sympy import Function, Derivative, var, simplify\n",
    "from sympy.abc import x, y, z, t\n",
    "from sympy import diff, log, pi\n",
    "s1 = Function(\"s1\")(x,y)\n",
    "s2 = Function(\"s2\")(x,y)\n",
    "F = Function(\"F\")(z,t)\n",
    "F1 = Derivative(F, z)\n",
    "F2 = Derivative(F, t)\n",
    "expr = -((s1 - z) * F1 + (s2 - t)*F2)\n",
    "print (diff(expr, z))\n",
    "print (diff(expr, t))\n",
    "x, eta1, eta2 = var('x eta1 eta2')\n",
    "s1 = x\n",
    "s2 = -x*x\n",
    "ld = log(2*(- eta1*eta1 - eta2))\n",
    "F =  - 0.5 * (1 + log(pi) + ld)\n",
    "dF1 = diff(F, eta1)\n",
    "dF2 = diff(F, eta2)\n",
    "dF11 = diff(dF1, eta1)\n",
    "dF12 = diff(dF1, eta2)\n",
    "dF21 = diff(dF2, eta1)\n",
    "dF22 = diff(dF2, eta2)\n",
    "print (dF11)\n",
    "print (dF12)\n",
    "print (dF21)\n",
    "print (dF22)\n",
    "print (-simplify(dF11*(s1-eta1)+dF12*(s2-eta2)))\n",
    "print (-simplify(dF21*(s1-eta1)+dF22*(s2-eta2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debut des manips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points stationnaires de $C_N(\\theta)$ par dérivation exacte\n",
    "\n",
    "   $$\\nabla C_N(\\theta) = 0 \\equiv -N^{-1} \\sum_i (s(x_i) - \\nabla F(\\theta)) = 0 \\equiv  \\nabla F(\\theta) = N^{-1} \\sum_i s(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradF_pt_stat_1 = np.sum(s_1(x) for x in Xt) / Nt\n",
    "gradF_pt_stat_2 = np.sum(s_2(x) for x in Xt) / Nt\n",
    "pt_stat = gradF_pt_stat_1, - gradF_pt_stat_1**2 - gradF_pt_stat_2\n",
    "print ((mu_true, sigma2_true), ' vs ', pt_stat)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xtest))\n",
    "gradF_pt_stat_1 = np.cumsum([s_1(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "gradF_pt_stat_2 = np.cumsum([s_2(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "mu_list = gradF_pt_stat_1\n",
    "sig_list = -gradF_pt_stat_1**2 -gradF_pt_stat_2\n",
    "ave_ll_train = [ave_ll(mu, sig, Xt) for mu,sig in zip(mu_list, sig_list)]\n",
    "ave_ll_test = [ave_ll(mu, sig, Xtest) for mu,sig in zip(mu_list, sig_list)]\n",
    "this_result = [ave_ll_train, ave_ll_test, mu_list, sig_list, 'Exact']\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disp_results_all([this_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun_C_N(mu_sigma2):\n",
    "    return C_N(mu_sigma2[0], mu_sigma2[1], Xt)\n",
    "np.random.rand(seed)\n",
    "x0 = lb_init[0]\n",
    "bnds = ((-np.inf, np.inf), (1e-6, np.inf))   # variance is positive\n",
    "res_C_N = sco.minimize(fun_C_N, x0, bounds=bnds) #, options={'gtol': 1e-6, 'disp': True})\n",
    "print(res_C_N)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (natural space)\n",
    "mostly fail:\n",
    "- $\\alpha = 0.01$\n",
    "\n",
    "success:\n",
    "- $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "epochs = 10\n",
    "theta1, theta2 = th_init[0]\n",
    "def grad_nll_1(x, theta1, theta2):\n",
    "    return - (s_1(x) - gradF_1_1D(theta1, theta2))\n",
    "\n",
    "def grad_nll_2(x, theta1, theta2):\n",
    "    return - (s_2(x) - gradF_2_1D(theta1, theta2))\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for xt in Xt:\n",
    "        #print(\"avant theta\",theta1, theta2, \" x \", xt)\n",
    "        #print (\"grad 1 \", gradF_1_1D(theta1, theta2), grad_nll_1(xt, theta1, theta2))\n",
    "        #print (\"grad 2 \", gradF_2_1D(theta1, theta2), grad_nll_2(xt, theta1, theta2))\n",
    "        theta1 -= alpha*grad_nll_1(xt, theta1, theta2)\n",
    "        theta2 -= alpha*grad_nll_2(xt, theta1, theta2)\n",
    "        #print(\"apres theta\", theta1, theta2)\n",
    "        mu_est, sigma2_est = th2lb(theta1,theta2)\n",
    "        #print(\"apres mu_sd2\",   mu_est, sigma2_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, \n",
    "               'SGD (Theta) - CSJ '+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results_all(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  \n",
    "\n",
    "Il s'agit d'un réécriture simple en remplacant $\\nabla F$ par $\\eta$. \n",
    "\n",
    "Cela ressemble dans la forme à l'approximation stochastique du Online EM.\n",
    "\n",
    "L'algorihme converge car les deux optimisations (avec celles du dessus) sont liées. \n",
    "\n",
    "(J'ai fait le calcul $\\eta^{(n+1)} - \\eta^{(n)}$ vers $\\theta^{(n+1)} - \\theta^{(n)}$)\n",
    "\n",
    "Il ne correspond pas à un SGD dand $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_nll_1(x, eta1, eta2):\n",
    "    return -(s_1(x) - eta1)\n",
    "\n",
    "def grad_nll_2(x, eta1, eta2):\n",
    "    return -(s_2(x) - eta2)\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_nll_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_nll_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(M)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (Expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "eta1, eta2 = et_init[0]\n",
    "def grad_1(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta2 + x**2) + (eta1 - x)*(8.0*eta1**2 - 8.0*eta2)/8.)/(eta1**2 + eta2)**2\n",
    "\n",
    "def grad_2(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD (Eta) - CSJ'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_lb(mu_sigma2, update_method, **kwargs):\n",
    "    mu_0, sigma2_0 = mu_sigma2\n",
    "    x = T.scalar()\n",
    "    mu = theano.shared(mu_0, name=\"mu\")\n",
    "    sigma2 = theano.shared(sigma2_0, name=\"sigma2\")\n",
    "    cost_lb =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "    params = [mu, sigma2]\n",
    "    ests, updates = update_method(cost_lb, params, **kwargs)\n",
    "    train_lb = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_lb(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = ests[0].get_value(), ests[1].get_value()\n",
    "        else:\n",
    "            mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]\n",
    "\n",
    "def train_on_th(th1_th2, update_method, **kwargs):\n",
    "    th1_0, th2_0 = th1_th2\n",
    "    x = T.scalar()\n",
    "    theta1 = theano.shared(th1_0, name=\"theta1\")\n",
    "    theta2 = theano.shared(th2_0, name=\"theta2\")\n",
    "    cost_th =  - (x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [theta1, theta2]\n",
    "    ests, updates = update_method(cost_th, params, **kwargs)\n",
    "    train_th = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_th(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = th2lb(ests[0].get_value(), ests[1].get_value())\n",
    "        else:\n",
    "            mu_est, sigma2_est = th2lb(theta1.get_value(), theta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]\n",
    "\n",
    "def train_on_et(et1_et2, update_method, **kwargs):\n",
    "    et1_0, et2_0 = et1_et2\n",
    "    x = T.scalar()\n",
    "    eta1 = theano.shared(et1_0, name=\"eta1\")\n",
    "    eta2 = theano.shared(et2_0, name=\"eta2\")\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    cost_et =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [eta1, eta2]\n",
    "    ests, updates = update_method(cost_et, params, **kwargs)\n",
    "    train_et = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_et(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = et2lb(ests[0].get_value(), ests[1].get_value())\n",
    "        else:\n",
    "            mu_est, sigma2_est = et2lb(eta1.get_value(), eta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem = Memory(cachedir='/tmp/joblib')\n",
    "train_on_lb_cache = mem.cache(train_on_lb)\n",
    "train_on_th_cache = mem.cache(train_on_th)\n",
    "train_on_et_cache = mem.cache(train_on_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_updates(cost, params, alpha):\n",
    "    updates = [(param, param - alpha*T.grad(cost, param)) for param in params]\n",
    "    return None, updates\n",
    "\n",
    "def gradient_updates_2(cost, params, alpha):\n",
    "    t = theano.shared(1.)\n",
    "    updates= [(t, t+1)]\n",
    "    updates.extend([(param, param - np.power(t,-alpha)*T.grad(cost, param)) \n",
    "                    for param in params])\n",
    "    return None, updates\n",
    "\n",
    "def gradient_updates_momentum(cost, params, alpha, momentum):\n",
    "    \"\"\"\n",
    "    http://caffe.berkeleyvision.org/tutorial/solver.html\n",
    "    \"\"\"\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    updates = []\n",
    "    for param in params:\n",
    "        V = theano.shared(param.get_value()*0.)\n",
    "        updates.append((param, param + V))\n",
    "        updates.append((V, momentum*V - alpha * T.grad(cost, param)))\n",
    "    return None, updates\n",
    "\n",
    "# on peut rajouter d'autres méthodes ici\n",
    "\n",
    "def gradient_updates_average(cost, params, alpha, t0):\n",
    "    updates = []\n",
    "    ave_est = []\n",
    "    t = theano.shared(0.)\n",
    "    updates.append((t, t+1))   \n",
    "    for param in params:\n",
    "        ave = theano.shared(0.)\n",
    "        ave_est.append(ave)\n",
    "        updates.append((ave,\n",
    "                        T.switch((t+1) <= t0, param - alpha*T.grad(cost, param),\n",
    "                                 (t-t0)/(t-t0+1.) * ave +\n",
    "                                 (param - alpha*T.grad(cost, param)) / (t-t0+1.))\n",
    "                       ))\n",
    "        updates.append((param, param - alpha*T.grad(cost, param)))\n",
    "    return ave_est, updates\n",
    "\n",
    "def gradient_updates_adam(cost, params, alpha, beta1, beta2):\n",
    "    assert beta1 < 1 and beta1 >= 0\n",
    "    assert beta2 < 1 and beta2 >= 0\n",
    "    updates = []\n",
    "    t = theano.shared(1.)\n",
    "    updates.append((t, t+1))\n",
    "    for param in params:\n",
    "        gt = T.grad(cost, param) \n",
    "        mt = theano.shared(0.)\n",
    "        updates.append((mt, beta1 * mt + (1-beta1) * gt))\n",
    "        vt = theano.shared(0.)\n",
    "        updates.append((vt, beta2 * vt + (1-beta2) * gt * gt))\n",
    "        alpha_t = theano.shared(0.)\n",
    "        updates.append((alpha_t, \n",
    "                        alpha*T.sqrt(1 - beta2**t)/(1 - beta1**t)))\n",
    "        updates.append((param, param - alpha_t*mt/(T.sqrt(vt + 1e-8))))\n",
    "    return None, updates   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization SGD via Theano (source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this_result = train_on_lb(lb_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_average, alpha=0.001,t0=500)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_average, alpha=0.001,t0=500)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_average, alpha=0.1,t0=500)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "print(this_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "#for alpha in alphas:\n",
    "#    for lb in lb_init:\n",
    "#        train_on_lb_cache(lb, gradient_updates, alpha=alpha)\n",
    "Parallel(n_jobs=-1, verbose=5)(delayed(train_on_lb_cache)(lb, gradient_updates, alpha=alpha)\n",
    "                               for alpha, lb in it.product(alphas, lb_init))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_0005 = [train_on_lb_cache(lb, gradient_updates, alpha=0.005) for lb in lb_init]\n",
    "#ll_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#ll_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#results_001 = [train_on_lb_cache(lb, gradient_updates, alpha=0.01) for lb in lb_init]\n",
    "#ll_boxplot(results_001, Xt, [200, 2000, 20000])\n",
    "kl_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#ll_boxplot(results_001, Xt, [200, 2000, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
