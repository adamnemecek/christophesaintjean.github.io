{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/bin/../lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "import site, os\n",
    "try:\n",
    "    print(site.getsitepackages())\n",
    "except:\n",
    "    print(os.path.dirname(site.__file__) + '/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "%matplotlib qt4\n",
    "#print (matplotlib.rcsetup.interactive_bk)\n",
    "#print (matplotlib.rcsetup.non_interactive_bk)\n",
    "#print (matplotlib.rcsetup.all_backends)\n",
    "print (matplotlib.get_backend())\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '24217' (I am process '27129')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/csaintje/.theano/compiledir_Darwin-15.4.0-x86_64-i386-64bit-i386-3.5.1-64/lock_dir\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for gaussian distribution as an exponential family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lb2th(mu,sigma2):\n",
    "    return mu / sigma2, 0.5 / sigma2\n",
    "\n",
    "def lb2et(mu, sigma2):\n",
    "    return mu, -(mu**2+sigma2)\n",
    "\n",
    "def th2lb(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5 / th2\n",
    "\n",
    "def th2et(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5*(-0.5*th1**2/th2 - 1) / th2\n",
    "\n",
    "def et2lb(et1,et2):\n",
    "    return et1, -(et1**2+et2)\n",
    "\n",
    "def et2th(et1,et2):\n",
    "    return - et1 /(et1**2 + et2), -0.5 /(et1**2 + et2)\n",
    "\n",
    "def s_1(x):\n",
    "    return x\n",
    "\n",
    "def s_2(x):\n",
    "    return -x*x\n",
    "\n",
    "def F_1D(theta1,theta2):\n",
    "    return 0.25*theta1*theta1/theta2 + 0.5*np.log(np.pi) - 0.5*np.log(theta2) \n",
    "\n",
    "def gradF_1_1D(theta_1,theta_2):\n",
    "    return 0.5*theta_1/theta_2\n",
    "\n",
    "def gradF_2_1D(theta_1, theta_2):\n",
    "    temp_1 = 0.5 / theta_2\n",
    "    temp_2 = temp_1 * theta_1\n",
    "    return -1. * (temp_2 * temp_2 + temp_1)\n",
    "\n",
    "def gradF_1_nD(theta_1,theta_2):\n",
    "    return 0.5*np.dot(np.inv(theta_2), theta_1)\n",
    "\n",
    "def gradF_2_nD(theta_1, theta_2):\n",
    "    temp_1 = 0.5*np.inv(theta_2)\n",
    "    temp_2 = np.dot(temp1,theta_1)\n",
    "    return - np.outer(temp_2,temp_2) - temp_1\n",
    "\n",
    "def gradG_1_1D(eta_1,eta_2):\n",
    "    return eta1 / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_2_1D(eta_1,eta_2):\n",
    "    return 0.5  / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_1_nD(eta_1,eta_2):\n",
    "    return np.dot(np.inv(-np.outer(eta_1, eta1) - eta2), eta1)\n",
    "\n",
    "def gradG_2_nD(eta_1,eta_2):\n",
    "    return 0.5*np.inv(-np.outer(eta_1, eta1) - eta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas 1D - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "N, batch_size = 2000, 100\n",
    "mu_true, sigma_true = 1, 2\n",
    "sigma2_true = sigma_true**2\n",
    "theta1_true, theta2_true = lb2th(mu_true, sigma2_true)\n",
    "eta1_true, eta2_true = lb2et(mu_true, sigma2_true)\n",
    "X = np.random.normal(mu_true,np.sqrt(sigma2_true), N)\n",
    "replicate = 5\n",
    "mu_0, sigma2_0 = 0, 10\n",
    "a_0 = 1 \n",
    "lb_init = [(mu, sigma2) for mu, sigma2 in zip(np.random.normal(mu_0,np.sqrt(sigma2_0), replicate),\n",
    "                                                  scs.invgamma.rvs(a_0, size=replicate))]\n",
    "th_init = [lb2th(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "et_init = [lb2et(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "#print(lb_init, th_init, et_init)\n",
    "def batches(Y, bs):\n",
    "    nb = np.int(np.ceil(len(Y) / bs))\n",
    "    Yb = [Y[i * bs : (i+1) * bs] for i in range(nb)]\n",
    "    def __temp(i):\n",
    "        return Yb[i]\n",
    "    return nb, Yb, __temp\n",
    "\n",
    "Xt = X[: N//4]\n",
    "Nt = len(Xt)\n",
    "NtB, XtB, XtB_f = batches(Xt, batch_size)\n",
    "Xv = X[N//4 : N//2]\n",
    "Nv = len(Xv)\n",
    "NvB, XvB, XvB_f = batches(Xv, batch_size)\n",
    "Xtest = X[N//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(x, mu, sigma2):\n",
    "    return -(x - mu)**2 /(2 * sigma2) - np.log(np.sqrt(2 * sigma2 * np.pi)) \n",
    "\n",
    "def ll_theta(x, theta1, theta2):\n",
    "    return s_1(x) * theta1 + s_2(x) * theta2 - F_1D(theta1, theta2)\n",
    "\n",
    "def ll_eta(x, eta1, eta2):\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    return ll_theta(x, theta1, theta2)\n",
    "    \n",
    "\n",
    "def ave_ll(mu, sigma2, chi):\n",
    "    N = len(chi)\n",
    "    return (1. / N) * sum(ll(x, mu, sigma2) for x in chi)\n",
    "    \n",
    "def C_N(mu, sigma2, chi):\n",
    "    return -ave_ll(mu, sigma2, chi)\n",
    "\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_theta(4, theta1_true, theta2_true))\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_eta(4, eta1_true, eta2_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "def disp_results(results):\n",
    "    n = len(results)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n))\n",
    "    table = ListTable()\n",
    "    table.append(['', 'Train', 'Test', 'mu', 'sigma^2'])\n",
    "    table.append(['True', ave_ll(mu_true, sigma2_true, Xt), \n",
    "                  ave_ll(mu_true, sigma2_true, Xtest),\n",
    "                  str(mu_true), str(sigma_true**2)])\n",
    "    for c, result in enumerate(results):\n",
    "        train, test, mu, sig, method = result\n",
    "        axes[0,0].plot(train, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,0].set_title('Average likelihood on train set')\n",
    "        axes[0,0].legend(loc=4)\n",
    "        axes[0,1].plot(test, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,1].set_title('Average likelihood on test set')\n",
    "        axes[0,1].legend(loc=4)\n",
    "        axes[1,0].plot(mu, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,0].set_title('Estimates of $\\mu$')\n",
    "        axes[1,0].legend(loc=4)\n",
    "        axes[1,1].plot(sig, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,1].set_title('Estimates of $\\sigma^2$')\n",
    "        axes[1,1].legend(loc=4)\n",
    "        table.append([method, train[-1], test[-1],mu[-1], sig[-1]])\n",
    "    display(HTML(table._repr_html_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem = joblib.Memory(cachedir='/tmp/joblib')\n",
    "\n",
    "import Parallel, delayed\n",
    ">>> from math import sqrt\n",
    ">>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
    "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "def result_results(fun, **kwargs):\n",
    "    \n",
    "    mem = Memory(cachedir='/tmp/joblib')\n",
    "    \n",
    "\n",
    "from joblib import Memory\n",
    ">>> mem = Memory(cachedir='/tmp/joblib')\n",
    ">>> import numpy as np\n",
    ">>> a = np.vander(np.arange(3)).astype(np.float)\n",
    ">>> square = mem.cache(np.square)\n",
    ">>> b = square(a)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points stationnaires de $C(\\theta) = \\mathbb{E}_{\\pi} [C(\\theta,x)] = \\mathbb{E}_{\\pi} [-\\log p(x;\\theta)]$ par Robbins-Monro\n",
    "\n",
    "\n",
    "### Robbins-Monro\n",
    "\n",
    "On dispose d'une fonction inconnue (supposée monotone) $M(\\theta)$ telle que \n",
    "$$M(\\theta) = \\mathbb{E}_{\\pi(\\beta|\\theta)} [\\beta]$$ \n",
    "avec $\\beta$ une v.a désignant des observations bruitées de $M(\\theta)$.\n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\alpha$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} - \\theta^{(t)} = a^{(t)} (\\alpha - \\beta^{(t)})$$\n",
    "\n",
    "### Robbins-Monro  et gradient stochastique\n",
    "\n",
    "$M(\\theta) := \\nabla_{\\theta} C(\\theta)$ est le gradient d'une fonction inconnue $C$. \n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\nabla C(\\theta^*) = 0$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} = \\theta^{(t)} - a^{(t)} \\beta^{(t)}$$\n",
    "où $\\beta^{(t)}$ est une observation bruitée de $\\nabla_{\\theta} C(\\theta^{(t)})$.\n",
    "\n",
    "### Pour notre cas\n",
    "La fonction à minimiser est:\n",
    "$$C(\\theta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\theta)]$$\n",
    "où $\\pi$ est la distribution inconnue dont on cherche une approximation $p$ paramétrée par $\\theta$ (identique à la minimisation sur $\\theta$ de $KL(\\pi || p(.;\\theta))$)\n",
    "\n",
    "Son équivalent en discret:\n",
    "$$C_N(\\theta) = - N^{-1} \\sum_i \\log p(x_i;\\theta)$$\n",
    "avec $\\lim_{N \\rightarrow +\\infty} C_N(\\theta) = C(\\theta)$ \n",
    "\n",
    "Sous conditions de regularité et dans la famille exponentielle, \n",
    "$$\\nabla_{\\theta} C(\\theta) = \\mathbb{E}_{\\pi} [- \\nabla_{\\theta}\\log p(x;\\theta)]  = \\mathbb{E}_{\\pi} [- s(x) + \\nabla_{\\theta} F (\\theta)]$$\n",
    "\n",
    "Pour le relier à Robbin-Monro, on a une observation bruitée\n",
    "$$\\beta^{(t)} = - s(x^{(t)}) + \\nabla_{\\theta} F (\\theta^{(t)})$$\n",
    "et donc la suite convergente $$\\theta^{(t+1)} = \\theta^{(t)} + a^{(t)} (s(x^{(t)}) - \\nabla_{\\theta} F (\\theta^{(t)}))$$\n",
    "\n",
    "Est ce que la formulation suivante est équivalente ? sans doute non ...\n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} (s(x^{(t)}) - \\eta^{(t)}))$$\n",
    "\n",
    "Dans l'espace $H$ (paramètre d'espérance), la même optimisation:\n",
    "$$C(\\eta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\eta)] = \\mathbb{E}_{\\pi} [B_{F^*}(s(x) : \\eta) - F^*(s(x)) - k(x)]$$\n",
    "$$\\nabla_{\\eta} C(\\eta) = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} B_{F^*}(s(x) : \\eta)] = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} (F^*(s(x)) - F^*(\\eta) - <s(x) - \\eta, \\nabla_\\eta F^*(\\eta)>)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) - [-1,0;0,-1]*\\nabla_\\eta F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) + \\nabla_{\\eta} F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)] = \\mathbb{E}_{\\pi} [- Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "et donc la suite convergente \n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} Hess F^*(\\eta^{(t)}) (s(x^{(t)}) - \\eta^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sympy import Function, Derivative, var, simplify\n",
    "from sympy.abc import x, y, z, t\n",
    "from sympy import diff, log, pi\n",
    "s1 = Function(\"s1\")(x,y)\n",
    "s2 = Function(\"s2\")(x,y)\n",
    "F = Function(\"F\")(z,t)\n",
    "F1 = Derivative(F, z)\n",
    "F2 = Derivative(F, t)\n",
    "expr = -((s1 - z) * F1 + (s2 - t)*F2)\n",
    "print (diff(expr, z))\n",
    "print (diff(expr, t))\n",
    "x, eta1, eta2 = var('x eta1 eta2')\n",
    "s1 = x\n",
    "s2 = -x*x\n",
    "ld = log(2*(- eta1*eta1 - eta2))\n",
    "F =  - 0.5 * (1 + log(pi) + ld)\n",
    "dF1 = diff(F, eta1)\n",
    "dF2 = diff(F, eta2)\n",
    "dF11 = diff(dF1, eta1)\n",
    "dF12 = diff(dF1, eta2)\n",
    "dF21 = diff(dF2, eta1)\n",
    "dF22 = diff(dF2, eta2)\n",
    "print (dF11)\n",
    "print (dF12)\n",
    "print (dF21)\n",
    "print (dF22)\n",
    "print (-simplify(dF11*(s1-eta1)+dF12*(s2-eta2)))\n",
    "print (-simplify(dF21*(s1-eta1)+dF22*(s2-eta2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debut des manips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points stationnaires de $C_N(\\theta)$ par dérivation exacte\n",
    "\n",
    "   $$\\nabla C_N(\\theta) = 0 \\equiv -N^{-1} \\sum_i (s(x_i) - \\nabla F(\\theta)) = 0 \\equiv  \\nabla F(\\theta) = N^{-1} \\sum_i s(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradF_pt_stat_1 = np.sum(s_1(x) for x in Xt) / Nt\n",
    "gradF_pt_stat_2 = np.sum(s_2(x) for x in Xt) / Nt\n",
    "pt_stat = gradF_pt_stat_1, - gradF_pt_stat_1**2 - gradF_pt_stat_2\n",
    "print ((mu_true, sigma2_true), ' vs ', pt_stat)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xtest))\n",
    "gradF_pt_stat_1 = np.cumsum([s_1(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "gradF_pt_stat_2 = np.cumsum([s_2(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "mu_list = gradF_pt_stat_1\n",
    "sig_list = -gradF_pt_stat_1**2 -gradF_pt_stat_2\n",
    "ave_ll_train = [ave_ll(mu, sig, Xt) for mu,sig in zip(mu_list, sig_list)]\n",
    "ave_ll_test = [ave_ll(mu, sig, Xtest) for mu,sig in zip(mu_list, sig_list)]\n",
    "this_result = [ave_ll_train, ave_ll_test, mu_list, sig_list, 'Exact']\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results([this_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun_C_N(mu_sigma2):\n",
    "    return C_N(mu_sigma2[0], mu_sigma2[1], Xt)\n",
    "np.random.rand(seed)\n",
    "x0 = (np.random.randn(), np.random.random())\n",
    "bnds = ((-np.inf, np.inf), (1e-6, np.inf))   # variance is positive\n",
    "res_C_N = sco.minimize(fun_C_N, x0, bounds=bnds) #, options={'gtol': 1e-6, 'disp': True})\n",
    "print(res_C_N)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (natural space)\n",
    "mostly fail:\n",
    "- $\\alpha = 0.01$\n",
    "\n",
    "success:\n",
    "- $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "theta1, theta2 = mu_0 / sigma2_0, 0.5 / sigma2_0\n",
    "\n",
    "def grad_nll_1(x, theta1, theta2):\n",
    "    return - (s_1(x) - gradF_1_1D(theta1, theta2))\n",
    "\n",
    "def grad_nll_2(x, theta1, theta2):\n",
    "    return - (s_2(x) - gradF_2_1D(theta1, theta2))\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for xt in Xt:\n",
    "        #print(\"avant theta\",theta1, theta2, \" x \", xt)\n",
    "        #print (\"grad 1 \", gradF_1_1D(theta1, theta2), grad_nll_1(xt, theta1, theta2))\n",
    "        #print (\"grad 2 \", gradF_2_1D(theta1, theta2), grad_nll_2(xt, theta1, theta2))\n",
    "        theta1 -= alpha*grad_nll_1(xt, theta1, theta2)\n",
    "        theta2 -= alpha*grad_nll_2(xt, theta1, theta2)\n",
    "        #print(\"apres theta\", theta1, theta2)\n",
    "        mu_est, sigma2_est = 0.5*theta1/theta2, 0.5/theta2\n",
    "        #print(\"apres mu_sd2\",   mu_est, sigma2_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(T)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disp_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a1a569bdeee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#disp_results([this_result])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisp_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'disp_results' is not defined"
     ]
    }
   ],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  \n",
    "\n",
    "Il s'agit d'un réécriture simple en remplacant $\\nabla F$ par $\\eta$. \n",
    "\n",
    "Cela ressemble dans la forme à l'approximation stochastique du Online EM.\n",
    "\n",
    "L'algorihme converge car les deux optimisations (avec celles du dessus) sont liées. \n",
    "\n",
    "(J'ai fait le calcul $\\eta^{(n+1)} - \\eta^{(n)}$ vers $\\theta^{(n+1)} - \\theta^{(n)}$)\n",
    "\n",
    "Il ne correspond pas à un SGD dand $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_nll_1(x, eta1, eta2):\n",
    "    return -(s_1(x) - eta1)\n",
    "\n",
    "def grad_nll_2(x, eta1, eta2):\n",
    "    return -(s_2(x) - eta2)\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_nll_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_nll_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(M)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (Expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_1(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta2 + x**2) + (eta1 - x)*(8.0*eta1**2 - 8.0*eta2)/8.)/(eta1**2 + eta2)**2\n",
    "\n",
    "def grad_2(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(H)-CSJ'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_lb(mu_sigma2, update_method, **kwargs):\n",
    "    mu_0, sigma2_0 = mu_sigma2\n",
    "    x = T.scalar()\n",
    "    mu = theano.shared(mu_0, name=\"mu\")\n",
    "    sigma2 = theano.shared(sigma2_0, name=\"sigma2\")\n",
    "    cost_lb =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "    params = [mu, sigma2]\n",
    "    updates = update_method(cost_lb, params, **kwargs)\n",
    "    aux = False\n",
    "    if len(updates) > 1:\n",
    "        aux_est, updates = updates\n",
    "        aux = True\n",
    "    train_lb = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "    for xt in Xt:\n",
    "        train_lb(xt)\n",
    "        if aux:\n",
    "            mu_est, sigma2_est = aux_est[0].get_value(), aux_est[1].get_value()\n",
    "        else:\n",
    "            mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "    return [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list]\n",
    "\n",
    "def train_on_th(th1_th2, update_method, **kwargs):\n",
    "    th1_0, th2_0 = th1_th2\n",
    "    x = T.scalar()\n",
    "    theta1 = theano.shared(th1_0, name=\"theta1\")\n",
    "    theta2 = theano.shared(th2_0, name=\"theta2\")\n",
    "    cost_th =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [theta1, theta2]\n",
    "    train_th = theano.function(inputs=[x], updates=update_method(cost_th, params, **kwargs))\n",
    "    mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "    for xt in Xt:\n",
    "        train_th(xt)\n",
    "        mu_est, sigma2_est = th2lb(theta1.get_value(), theta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "    return [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list]\n",
    "\n",
    "def train_on_et(et1_et2, update_method, **kwargs):\n",
    "    et1_0, et2_0 = et1_et2\n",
    "    x = T.scalar()\n",
    "    eta1 = theano.shared(et1_0, name=\"eta1\")\n",
    "    eta2 = theano.shared(et2_0, name=\"eta2\")\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    cost_et =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [eta1, eta2]\n",
    "    train_et = theano.function(inputs=[x], updates=update_method(cost_et, params, **kwargs))\n",
    "    mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "    for xt in Xt:\n",
    "        train_th(xt)\n",
    "        mu_est, sigma2_est = et2lb(eta1.get_value(), eta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "    return [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem = joblib.Memory(cachedir='/tmp/joblib')\n",
    "train_on_lb_cache = mem.cache(train_on_lb)\n",
    "train_on_th_cache = mem.cache(train_on_th)\n",
    "train_on_et_cache = mem.cache(train_on_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_updates(cost, params, alpha):\n",
    "    updates = [(param, param - alpha*T.grad(cost, param)) for param in params]\n",
    "    return updates\n",
    "\n",
    "def gradient_updates_momentum(cost, params, alpha, momentum):\n",
    "    \"\"\"\n",
    "    http://caffe.berkeleyvision.org/tutorial/solver.html\n",
    "    \"\"\"\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    updates = []\n",
    "    for param in params:\n",
    "        V = theano.shared(param.get_value()*0.)\n",
    "        updates.append((param, param + V))\n",
    "        updates.append((V, momentum*V - alpha * T.grad(cost, param)))\n",
    "    return updates\n",
    "\n",
    "# on peut rajouter d'autres méthodes ici\n",
    "\n",
    "def gradient_updates_average(cost, params, alpha, t0):\n",
    "    # a verifier\n",
    "    updates = []\n",
    "    ave_est = []\n",
    "    t = theano.shared(0.)\n",
    "    updates.append((t, t+1))   \n",
    "    for param in params:\n",
    "        ave = theano.shared(0.)\n",
    "        ave_est.append(ave)\n",
    "        updates.append((ave, (t <= t0) * (param - alpha*T.grad(cost, param)) + \n",
    "                        (t > t0) * ((t-t0)*ave + (param - alpha*T.grad(cost, param)))/(t-t0+1)))\n",
    "        updates.append((param, param - alpha*T.grad(cost, param)))\n",
    "    return updates, ave_est\n",
    "\n",
    "def gradient_updates_adam(cost, params, alpha, beta1, beta2):\n",
    "    assert beta1 < 1 and beta1 >= 0\n",
    "    assert beta2 < 1 and beta2 >= 0\n",
    "    updates = []\n",
    "    t = theano.shared(1.)\n",
    "    updates.append((t, t+1))\n",
    "    for param in params:\n",
    "        gt = T.grad(cost, param) \n",
    "        mt = theano.shared(0.)\n",
    "        updates.append((mt, beta1 * mt + (1-beta1) * gt))\n",
    "        vt = theano.shared(0.)\n",
    "        updates.append((vt, beta2 * vt + (1-beta2) * gt * gt))\n",
    "        alpha_t = theano.shared(0.)\n",
    "        updates.append((alpha_t, \n",
    "                        alpha*T.sqrt(1 - beta2**t)/(1 - beta1**t)))\n",
    "        updates.append((param, param - alpha_t*mt/(T.sqrt(vt + 1e-8))))\n",
    "    return updates   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization SGD via Theano (source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-35e79a587363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthis_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_lb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mthis_result2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_lb_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mthis_result3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_lb_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-581c2f80bea8>\u001b[0m in \u001b[0;36mtrain_on_lb\u001b[0;34m(mu_sigma2, update_method, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_lb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_sigma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmu_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma2_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu_sigma2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msigma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigma2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "this_result = train_on_lb(lb_init[0], gradient_updates, alpha=0.001)\n",
    "this_result2 = train_on_lb_cache(lb_init[0], gradient_updates, alpha=0.001)\n",
    "this_result3 = train_on_lb_cache(lb_init[0], gradient_updates, alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of arguments for train_on_lb(Xt, mu_sigma2, update_method, **kwargs):\n     train_on_lb((-1.2522472864061529, 0.60546517297663849), <function gradient_updates at 0x11217b0d0>, alpha=0.1) was called.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/func_inspect.py\u001b[0m in \u001b[0;36mfilter_args\u001b[0;34m(func, ignore_lst, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     \u001b[0marg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_defaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ed3be0c097d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_SGD_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_lb_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-ed3be0c097d0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_SGD_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_lb_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# Compare the function code with the previous to see if the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;31m# function code has changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_output_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;31m# FIXME: The statements below should be try/excepted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_get_output_dir\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \"\"\"\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0margument_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_argument_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         output_dir = os.path.join(self._get_func_dir(self.func),\n\u001b[1;32m    518\u001b[0m                                   argument_hash)\n",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_get_argument_hash\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_argument_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         return hashing.hash(filter_args(self.func, self.ignore,\n\u001b[0;32m--> 509\u001b[0;31m                                          args, kwargs),\n\u001b[0m\u001b[1;32m    510\u001b[0m                              coerce_mmap=(self.mmap_mode is not None))\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/lib/python3.5/site-packages/joblib/func_inspect.py\u001b[0m in \u001b[0;36mfilter_args\u001b[0;34m(func, ignore_lst, args, kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0;34m'     %s was called.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         % (_signature_str(name, arg_spec),\n\u001b[0;32m--> 287\u001b[0;31m                            _function_called_str(name, args, kwargs))\n\u001b[0m\u001b[1;32m    288\u001b[0m                     )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of arguments for train_on_lb(Xt, mu_sigma2, update_method, **kwargs):\n     train_on_lb((-1.2522472864061529, 0.60546517297663849), <function gradient_updates at 0x11217b0d0>, alpha=0.1) was called."
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "results_SGD_source = [train_on_lb_cache(lb_init[0], gradient_updates, alpha=alpha) for alpha in alphas] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization SGD via Theano (source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll, [mu, sigma2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        __ = train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(S)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization SGD via Theano (natural space)\n",
    "\n",
    "Fails for $\\alpha =0.005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random() \n",
    "theta1 = theano.shared(mu_0 / sigma2_0, name=\"theta1\") # ensure initial value is positive\n",
    "theta2 = theano.shared(0.5 / sigma2_0, name=\"theta2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll_theta =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll_theta, [theta1, theta2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est = 0.5*theta1.get_value()/theta2.get_value()\n",
    "        sigma2_est = 0.5/theta2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(T)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization SGD via Theano (expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1 = theano.shared(mu_0, name=\"eta1\") # ensure initial value is positive\n",
    "eta2 = theano.shared(-(mu_0*mu_0 + sigma2_0), name=\"eta2\") # ensure initial value is positive\n",
    "print(eta1.get_value(), eta2.get_value())\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "nll_eta =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates(nll_eta, [eta1, eta2], alpha))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        # print(g1, s_1(xt) - eta1.get_value())\n",
    "        # print(g2, s_2(xt) - eta2.get_value())\n",
    "        mu_est = eta1.get_value()\n",
    "        sigma2_est = - (eta1.get_value()*eta1.get_value()+eta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(H)- Theano '+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization via Theano +  SGD Nesterov momemtum (Source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates_momentum(nll, [mu, sigma2], alpha, momentum))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'Mom. SGD(S)-'+str(momentum)+','+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization via Theano +  ADAM (Source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.005\n",
    "beta1 = 0.9\n",
    "beta2 = 0.995\n",
    "epochs = 10\n",
    "# Declare Theano symbolic variables\n",
    "x = T.scalar()\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "mu = theano.shared(mu_0, name=\"mu\")\n",
    "sigma2 = theano.shared(sigma2_0, name=\"sigma2\") # ensure initial value is positive\n",
    "\n",
    "# Construct Theano expression graph to minimize (- log-likehood for univariate gaussian)\n",
    "nll =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          updates=gradient_updates_adam(nll, [mu, sigma2], \n",
    "                                        alpha, beta1, beta2))\n",
    "\n",
    "# Train\n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for i in range(epochs):\n",
    "    for xt in Xt:\n",
    "        train(xt)\n",
    "        mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'Adam(S)-'+str(beta1)+','+str(beta2)+','+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
