{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csaintje/Documents/Recherche/Python/Envs/scientific_3_5/bin/../lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "import site, os\n",
    "try:\n",
    "    print(site.getsitepackages())\n",
    "except:\n",
    "    print(os.path.dirname(site.__file__) + '/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module://ipykernel.pylab.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#print (matplotlib.rcsetup.interactive_bk)\n",
    "#print (matplotlib.rcsetup.non_interactive_bk)\n",
    "#print (matplotlib.rcsetup.all_backends)\n",
    "print (matplotlib.get_backend())\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from joblib import Memory, Parallel, delayed, dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for gaussian distribution as an exponential family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lb2th(mu,sigma2):\n",
    "    return mu / sigma2, 0.5 / sigma2\n",
    "\n",
    "def lb2et(mu, sigma2):\n",
    "    return mu, -(mu**2+sigma2)\n",
    "\n",
    "def th2lb(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5 / th2\n",
    "\n",
    "def th2et(th1,th2):\n",
    "    return 0.5*th1/th2, 0.5*(-0.5*th1**2/th2 - 1) / th2\n",
    "\n",
    "def et2lb(et1,et2):\n",
    "    return et1, -(et1**2+et2)\n",
    "\n",
    "def et2th(et1,et2):\n",
    "    return - et1 /(et1**2 + et2), -0.5 /(et1**2 + et2)\n",
    "\n",
    "def s_1(x):\n",
    "    return x\n",
    "\n",
    "def s_2(x):\n",
    "    return -x*x\n",
    "\n",
    "def F_1D(theta1,theta2):\n",
    "    return 0.25*theta1*theta1/theta2 + 0.5*np.log(np.pi) - 0.5*np.log(theta2) \n",
    "\n",
    "def gradF_1_1D(theta_1,theta_2):\n",
    "    return 0.5*theta_1/theta_2\n",
    "\n",
    "def gradF_2_1D(theta_1, theta_2):\n",
    "    temp_1 = 0.5 / theta_2\n",
    "    temp_2 = temp_1 * theta_1\n",
    "    return -1. * (temp_2 * temp_2 + temp_1)\n",
    "\n",
    "def gradF_1_nD(theta_1,theta_2):\n",
    "    return 0.5*np.dot(np.inv(theta_2), theta_1)\n",
    "\n",
    "def gradF_2_nD(theta_1, theta_2):\n",
    "    temp_1 = 0.5*np.inv(theta_2)\n",
    "    temp_2 = np.dot(temp1,theta_1)\n",
    "    return - np.outer(temp_2,temp_2) - temp_1\n",
    "\n",
    "def gradG_1_1D(eta_1,eta_2):\n",
    "    return eta1 / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_2_1D(eta_1,eta_2):\n",
    "    return 0.5  / (-eta_1 * eta1 - eta2)\n",
    "\n",
    "def gradG_1_nD(eta_1,eta_2):\n",
    "    return np.dot(np.inv(-np.outer(eta_1, eta1) - eta2), eta1)\n",
    "\n",
    "def gradG_2_nD(eta_1,eta_2):\n",
    "    return 0.5*np.inv(-np.outer(eta_1, eta1) - eta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas 1D - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "N, batch_size = 20000*4, 100\n",
    "mu_true, sigma_true = 1, 2\n",
    "sigma2_true = sigma_true**2\n",
    "theta1_true, theta2_true = lb2th(mu_true, sigma2_true)\n",
    "eta1_true, eta2_true = lb2et(mu_true, sigma2_true)\n",
    "X = np.random.normal(mu_true,np.sqrt(sigma2_true), N)\n",
    "replicate = 100\n",
    "mu_0, sigma2_0 = 0, 10\n",
    "a_0 = 1 \n",
    "lb_init = [(mu, sigma2) for mu, sigma2 in \n",
    "           zip(np.random.normal(mu_0,np.sqrt(sigma2_0), replicate),\n",
    "               scs.invgamma.rvs(a_0, size=replicate))]\n",
    "th_init = [lb2th(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "et_init = [lb2et(mu, sigma2) for mu, sigma2 in lb_init]\n",
    "#print(lb_init, th_init, et_init)\n",
    "def batches(Y, bs):\n",
    "    nb = np.int(np.ceil(len(Y) / bs))\n",
    "    Yb = [Y[i * bs : (i+1) * bs] for i in range(nb)]\n",
    "    def __temp(i):\n",
    "        return Yb[i]\n",
    "    return nb, Yb, __temp\n",
    "\n",
    "Xt = X[: N//4]\n",
    "Nt = len(Xt)\n",
    "NtB, XtB, XtB_f = batches(Xt, batch_size)\n",
    "Xv = X[N//4 : N//2]\n",
    "Nv = len(Xv)\n",
    "NvB, XvB, XvB_f = batches(Xv, batch_size)\n",
    "Xtest = X[N//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(x, mu, sigma2):\n",
    "    #assert(sigma2 > 0)\n",
    "    return -(x - mu)**2 /(2 * sigma2) - np.log(np.sqrt(2 * sigma2 * np.pi)) \n",
    "\n",
    "def ll_theta(x, theta1, theta2):\n",
    "    return s_1(x) * theta1 + s_2(x) * theta2 - F_1D(theta1, theta2)\n",
    "\n",
    "def ll_eta(x, eta1, eta2):\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    return ll_theta(x, theta1, theta2)\n",
    "\n",
    "def kl(mu_1, s_1, mu_2, s_2):\n",
    "    #assert(s_1 > 0 and s_2 > 0)\n",
    "    val = 0.5*(np.log(s_2/s_1) + (s_1/s_2) + ((mu_1 - mu_2)**2 / s_2)  - 1)\n",
    "    return val\n",
    "    \n",
    "def ave_ll(mu, sigma2, chi):\n",
    "    N = len(chi)\n",
    "    #assert(sigma2 > 0)\n",
    "    return (1. / N) * sum(ll(x, mu, sigma2) for x in chi)\n",
    "    \n",
    "def C_N(mu, sigma2, chi):\n",
    "    #assert(sigma2 > 0)\n",
    "    return -ave_ll(mu, sigma2, chi)\n",
    "\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_theta(4, theta1_true, theta2_true))\n",
    "np.testing.assert_allclose(ll(4, mu_true, sigma2_true),\n",
    "                          ll_eta(4, eta1_true, eta2_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points stationnaires de $C(\\theta) = \\mathbb{E}_{\\pi} [C(\\theta,x)] = \\mathbb{E}_{\\pi} [-\\log p(x;\\theta)]$ par Robbins-Monro\n",
    "\n",
    "\n",
    "### Robbins-Monro\n",
    "\n",
    "On dispose d'une fonction inconnue (supposée monotone) $M(\\theta)$ telle que \n",
    "$$M(\\theta) = \\mathbb{E}_{\\pi(\\beta|\\theta)} [\\beta]$$ \n",
    "avec $\\beta$ une v.a désignant des observations bruitées de $M(\\theta)$.\n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\alpha$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} - \\theta^{(t)} = a^{(t)} (\\alpha - \\beta^{(t)})$$\n",
    "\n",
    "### Robbins-Monro  et gradient stochastique\n",
    "\n",
    "$M(\\theta) := \\nabla_{\\theta} C(\\theta)$ est le gradient d'une fonction inconnue $C$. \n",
    "\n",
    "On cherche la valeur $\\theta^*$ telle que $M(\\theta^*) = \\nabla C(\\theta^*) = 0$.\n",
    "\n",
    "Suite convergente de Robbins-Monro : $$\\theta^{(t+1)} = \\theta^{(t)} - a^{(t)} \\beta^{(t)}$$\n",
    "où $\\beta^{(t)}$ est une observation bruitée de $\\nabla_{\\theta} C(\\theta^{(t)})$.\n",
    "\n",
    "### Pour notre cas\n",
    "La fonction à minimiser est:\n",
    "$$C(\\theta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\theta)]$$\n",
    "où $\\pi$ est la distribution inconnue dont on cherche une approximation $p$ paramétrée par $\\theta$ (identique à la minimisation sur $\\theta$ de $KL(\\pi || p(.;\\theta))$)\n",
    "\n",
    "Son équivalent en discret:\n",
    "$$C_N(\\theta) = - N^{-1} \\sum_i \\log p(x_i;\\theta)$$\n",
    "avec $\\lim_{N \\rightarrow +\\infty} C_N(\\theta) = C(\\theta)$ \n",
    "\n",
    "Sous conditions de regularité et dans la famille exponentielle, \n",
    "$$\\nabla_{\\theta} C(\\theta) = \\mathbb{E}_{\\pi} [- \\nabla_{\\theta}\\log p(x;\\theta)]  = \\mathbb{E}_{\\pi} [- s(x) + \\nabla_{\\theta} F (\\theta)]$$\n",
    "\n",
    "Pour le relier à Robbin-Monro, on a une observation bruitée\n",
    "$$\\beta^{(t)} = - s(x^{(t)}) + \\nabla_{\\theta} F (\\theta^{(t)})$$\n",
    "et donc la suite convergente $$\\theta^{(t+1)} = \\theta^{(t)} + a^{(t)} (s(x^{(t)}) - \\nabla_{\\theta} F (\\theta^{(t)}))$$\n",
    "\n",
    "Est ce que la formulation suivante est équivalente ? sans doute non ...\n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} (s(x^{(t)}) - \\eta^{(t)}))$$\n",
    "\n",
    "Dans l'espace $H$ (paramètre d'espérance), la même optimisation:\n",
    "$$C(\\eta) = \\mathbb{E}_{\\pi} [- \\log p(x;\\eta)] = \\mathbb{E}_{\\pi} [B_{F^*}(s(x) : \\eta) - F^*(s(x)) - k(x)]$$\n",
    "$$\\nabla_{\\eta} C(\\eta) = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} B_{F^*}(s(x) : \\eta)] = \\mathbb{E}_{\\pi} [\\nabla_{\\eta} (F^*(s(x)) - F^*(\\eta) - <s(x) - \\eta, \\nabla_\\eta F^*(\\eta)>)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) - [-1,0;0,-1]*\\nabla_\\eta F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "$$ = \\mathbb{E}_{\\pi} [ - \\nabla_{\\eta} F^*(\\eta) + \\nabla_{\\eta} F^*(\\eta) - Hess F^*(\\eta) (s(x) - \\eta)] = \\mathbb{E}_{\\pi} [- Hess F^*(\\eta) (s(x) - \\eta)]$$\n",
    "et donc la suite convergente \n",
    "$$\\eta^{(t+1)} = \\eta^{(t)} + a^{(t)} Hess F^*(\\eta^{(t)}) (s(x^{(t)}) - \\eta^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debut des manips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-(-t + s2(x, y))*Derivative(F(z, t), t, z) - (-z + s1(x, y))*Derivative(F(z, t), z, z) + Derivative(F(z, t), z)\n",
      "-(-t + s2(x, y))*Derivative(F(z, t), t, t) - (-z + s1(x, y))*Derivative(F(z, t), t, z) + Derivative(F(z, t), t)\n",
      "------Grad--------\n",
      "2.0*eta1/(-2*eta1**2 - 2*eta2)\n",
      "1.0/(-2*eta1**2 - 2*eta2)\n",
      "----- Terms of hessian---------\n",
      "8.0*eta1**2/(-2*eta1**2 - 2*eta2)**2 + 2.0/(-2*eta1**2 - 2*eta2)\n",
      "4.0*eta1/(-2*eta1**2 - 2*eta2)**2\n",
      "4.0*eta1/(-2*eta1**2 - 2*eta2)**2\n",
      "2.0/(-2*eta1**2 - 2*eta2)**2\n",
      "----Hessian----------\n",
      "Matrix([\n",
      "[1.0*(-eta1**3 + eta1**2*s1(x, y) + eta1*s2(x, y) - eta2*s1(x, y))/(eta1**2 + eta2)**2],\n",
      "[     (-1.0*eta1**2 + 1.0*eta1*s1(x, y) - 0.5*eta2 + 0.5*s2(x, y))/(eta1**2 + eta2)**2]])\n",
      "-------Update Rule-------\n",
      "--------------\n",
      "(1.0*eta1*(eta2 + x**2) + 1.0*(eta1 - x)*(eta1**2 - eta2))/(eta1**2 + eta2)**2\n",
      "(1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n"
     ]
    }
   ],
   "source": [
    "from sympy import Function, Derivative, var, simplify, collect, expand, factor, Matrix\n",
    "from sympy.abc import x, y, z, t\n",
    "from sympy import diff, hessian, log, pi\n",
    "s1 = Function(\"s1\")(x,y)\n",
    "s2 = Function(\"s2\")(x,y)\n",
    "F = Function(\"F\")(z,t)\n",
    "F1 = Derivative(F, z)\n",
    "F2 = Derivative(F, t)\n",
    "expr = -((s1 - z) * F1 + (s2 - t)*F2)\n",
    "print (diff(expr, z))\n",
    "print (diff(expr, t))\n",
    "eta1, eta2 = var('eta1 eta2')\n",
    "ld = log(2*(- eta1*eta1 - eta2))\n",
    "F =  - 0.5 * (1 + log(pi) + ld)\n",
    "dF1 = diff(F, eta1)\n",
    "dF2 = diff(F, eta2)\n",
    "print('------Grad--------')\n",
    "print (dF1)\n",
    "print (dF2)\n",
    "print('----- Terms of hessian---------')\n",
    "dF11 = diff(dF1, eta1)\n",
    "dF12 = diff(dF1, eta2)\n",
    "dF21 = diff(dF2, eta1)\n",
    "dF22 = diff(dF2, eta2)\n",
    "print (dF11)\n",
    "print (dF12)\n",
    "print (dF21)\n",
    "print (dF22)\n",
    "print('----Hessian----------')\n",
    "#print(diff(F, eta1, eta1))\n",
    "J = simplify(factor(expand(hessian(F, [eta1, eta2]))))\n",
    "#print(J)\n",
    "M2 = Matrix(2,1,[s1-eta1, s2-eta2])\n",
    "print(simplify(factor(expand(J*M2))))\n",
    "print('-------Update Rule-------')\n",
    "# regle de mise à jour\n",
    "x  = var('x')\n",
    "s1 = x\n",
    "s2 = -x*x\n",
    "print('--------------')\n",
    "print (-simplify(dF11*(s1-eta1)+dF12*(s2-eta2)))\n",
    "print (-simplify(dF21*(s1-eta1)+dF22*(s2-eta2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points stationnaires de $C_N(\\theta)$ par dérivation exacte\n",
    "\n",
    "   $$\\nabla C_N(\\theta) = 0 \\equiv -N^{-1} \\sum_i (s(x_i) - \\nabla F(\\theta)) = 0 \\equiv  \\nabla F(\\theta) = N^{-1} \\sum_i s(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradF_pt_stat_1 = np.sum(s_1(x) for x in Xt) / Nt\n",
    "gradF_pt_stat_2 = np.sum(s_2(x) for x in Xt) / Nt\n",
    "pt_stat = gradF_pt_stat_1, - gradF_pt_stat_1**2 - gradF_pt_stat_2\n",
    "print ((mu_true, sigma2_true), ' vs ', pt_stat)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(pt_stat[0], pt_stat[1], Xtest))\n",
    "gradF_pt_stat_1 = np.cumsum([s_1(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "gradF_pt_stat_2 = np.cumsum([s_2(x) for x in Xt]) / np.arange(1,Nt+1)\n",
    "mu_list = gradF_pt_stat_1\n",
    "sig_list = -gradF_pt_stat_1**2 -gradF_pt_stat_2\n",
    "\n",
    "ave_ll_train = Parallel(n_jobs=8)(delayed(ave_ll)(mu, sig, Xt) for __,mu,sig in zip(range(len(mu_list)),\n",
    "                                                                                         mu_list, sig_list))\n",
    "ave_ll_test = Parallel(n_jobs=8)(delayed(ave_ll)(mu, sig, Xtest) for mu,sig in zip(mu_list, sig_list))\n",
    "KL_exact = Parallel(n_jobs=8)(delayed(kl)(mu_true, sigma2_true, mu, sig) for mu,sig in zip(mu_list, sig_list))\n",
    "#ave_ll_test = [ave_ll(mu, sig, Xtest) for mu,sig in zip(mu_list, sig_list)]\n",
    "#KL_exact = [kl(mu_true, sigma2_true, mu, sig) for mu,sig in zip(mu_list, sig_list)]\n",
    "#this_result = [ave_ll_train, ave_ll_test, mu_list, sig_list, 'Exact']\n",
    "#results.append(this_result)\n",
    "resultats_exact = [mu_list, sig_list, ave_ll_train, ave_ll_test, KL_exact]\n",
    "dump(resultats_exact, 'resultats_exact.pklz', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun_C_N(mu_sigma2):\n",
    "    return C_N(mu_sigma2[0], mu_sigma2[1], Xt)\n",
    "np.random.rand(seed)\n",
    "x0 = lb_init[0]\n",
    "bnds = ((-np.inf, np.inf), (1e-6, np.inf))   # variance is positive\n",
    "res_C_N = sco.minimize(fun_C_N, x0, bounds=bnds) #, options={'gtol': 1e-6, 'disp': True})\n",
    "print(res_C_N)\n",
    "print('average ll on training set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xt), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xt))\n",
    "print('average ll on test set : ', \n",
    "      ave_ll(mu_true, sigma2_true, Xtest), ' vs ',\n",
    "      ave_ll(res_C_N.x[0], res_C_N.x[1], Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (natural space)\n",
    "mostly fail:\n",
    "- $\\alpha = 0.01$\n",
    "\n",
    "success:\n",
    "- $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "epochs = 10\n",
    "theta1, theta2 = th_init[0]\n",
    "def grad_nll_1(x, theta1, theta2):\n",
    "    return - (s_1(x) - gradF_1_1D(theta1, theta2))\n",
    "\n",
    "def grad_nll_2(x, theta1, theta2):\n",
    "    return - (s_2(x) - gradF_2_1D(theta1, theta2))\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for xt in Xt:\n",
    "        #print(\"avant theta\",theta1, theta2, \" x \", xt)\n",
    "        #print (\"grad 1 \", gradF_1_1D(theta1, theta2), grad_nll_1(xt, theta1, theta2))\n",
    "        #print (\"grad 2 \", gradF_2_1D(theta1, theta2), grad_nll_2(xt, theta1, theta2))\n",
    "        theta1 -= alpha*grad_nll_1(xt, theta1, theta2)\n",
    "        theta2 -= alpha*grad_nll_2(xt, theta1, theta2)\n",
    "        #print(\"apres theta\", theta1, theta2)\n",
    "        mu_est, sigma2_est = th2lb(theta1,theta2)\n",
    "        #print(\"apres mu_sd2\",   mu_est, sigma2_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest)) \n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, \n",
    "               'SGD (Theta) - CSJ '+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_results_all(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  \n",
    "\n",
    "Il s'agit d'un réécriture simple en remplacant $\\nabla F$ par $\\eta$. \n",
    "\n",
    "Cela ressemble dans la forme à l'approximation stochastique du Online EM.\n",
    "\n",
    "L'algorihme converge car les deux optimisations (avec celles du dessus) sont liées. \n",
    "\n",
    "(J'ai fait le calcul $\\eta^{(n+1)} - \\eta^{(n)}$ vers $\\theta^{(n+1)} - \\theta^{(n)}$)\n",
    "\n",
    "Il ne correspond pas à un SGD dand $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 10\n",
    "mu_0, sigma2_0 = np.random.randn(), np.random.random()\n",
    "eta1, eta2 = mu_0 , -(mu_0*mu_0 + sigma2_0)\n",
    "def grad_nll_1(x, eta1, eta2):\n",
    "    return -(s_1(x) - eta1)\n",
    "\n",
    "def grad_nll_2(x, eta1, eta2):\n",
    "    return -(s_2(x) - eta2)\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        #print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_nll_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_nll_2(x, eta1, eta2)\n",
    "        #print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = eta1, -(eta1*eta1+eta2)\n",
    "        #print(\"apres mu_sd2\", mu_est, sd_est)\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD(M)-'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via Stochastic Gradient Descent  (Expectation space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant eta -7.7390047236 -60.3543879848  x  -0.424781324101\n",
      "apres eta 53.4754906879 -60.3543994871\n",
      "apres mu_sd2 53.4754906879 -2799.27370482\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8317ad3167ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apres mu_sd2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma2_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mave_ll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma2_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mmu_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msig_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d5405cbee774>\u001b[0m in \u001b[0;36mave_ll\u001b[0;34m(mu, sigma2, chi)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mave_ll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.0316\n",
    "epochs = 1\n",
    "eta1, eta2 = et_init[53]\n",
    "def grad_1(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta2 + x**2) + (eta1 - x)*(8.0*eta1**2 - 8.0*eta2)/8.)/(eta1**2 + eta2)**2\n",
    "\n",
    "def grad_2(x, eta1, eta2):\n",
    "    return (1.0*eta1*(eta1 - x) + 0.5*eta2 + 0.5*x**2)/(eta1**2 + eta2)**2\n",
    "    \n",
    "mu_list, sig_list, ave_ll_list_train, ave_ll_list_test = [], [], [], []\n",
    "for __ in range(epochs):\n",
    "    for x in Xt:\n",
    "        print(\"avant eta\",eta1, eta2, \" x \", x)\n",
    "        #print (\"grad 1 \", grad_nll_1(x, eta1, eta2))\n",
    "        #print (\"grad 2 \", grad_nll_2(x, eta1, eta2))\n",
    "        eta1 -= alpha*grad_1(x, eta1, eta2)\n",
    "        eta2 -= alpha*grad_2(x, eta1, eta2)\n",
    "        print(\"apres eta\", eta1, eta2)\n",
    "        mu_est, sigma2_est = et2lb(eta1,eta2)\n",
    "        #\n",
    "        print(\"apres mu_sd2\", mu_est, sigma2_est)\n",
    "        print(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "        ave_ll_list_train.append(ave_ll(mu_est, sigma2_est, Xt)) \n",
    "        ave_ll_list_test.append(ave_ll(mu_est, sigma2_est, Xtest))\n",
    "this_result = [ave_ll_list_train, ave_ll_list_test, mu_list, sig_list, 'SGD (Eta) - CSJ'+str(alpha)]\n",
    "results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#disp_results([this_result])\n",
    "disp_results(results[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_lb(mu_sigma2, update_method, **kwargs):\n",
    "    mu_0, sigma2_0 = mu_sigma2\n",
    "    x = T.scalar()\n",
    "    mu = theano.shared(mu_0, name=\"mu\")\n",
    "    sigma2 = theano.shared(sigma2_0, name=\"sigma2\")\n",
    "    cost_lb =  (x - mu)**2 /(2 * sigma2) + T.log(T.sqrt(2 * sigma2 *np.pi))\n",
    "    params = [mu, sigma2]\n",
    "    ests, updates = update_method(cost_lb, params, **kwargs)\n",
    "    train_lb = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_lb(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = ests[0].get_value(), ests[1].get_value()\n",
    "        else:\n",
    "            mu_est, sigma2_est = mu.get_value(), sigma2.get_value()\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]\n",
    "\n",
    "def train_on_th(th1_th2, update_method, **kwargs):\n",
    "    th1_0, th2_0 = th1_th2\n",
    "    x = T.scalar()\n",
    "    theta1 = theano.shared(th1_0, name=\"theta1\")\n",
    "    theta2 = theano.shared(th2_0, name=\"theta2\")\n",
    "    cost_th =  - (x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [theta1, theta2]\n",
    "    ests, updates = update_method(cost_th, params, **kwargs)\n",
    "    train_th = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_th(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = th2lb(ests[0].get_value(), ests[1].get_value())\n",
    "        else:\n",
    "            mu_est, sigma2_est = th2lb(theta1.get_value(), theta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]\n",
    "\n",
    "def train_on_et(et1_et2, update_method, **kwargs):\n",
    "    et1_0, et2_0 = et1_et2\n",
    "    x = T.scalar()\n",
    "    eta1 = theano.shared(et1_0, name=\"eta1\")\n",
    "    eta2 = theano.shared(et2_0, name=\"eta2\")\n",
    "    theta1 = - eta1 / (eta1*eta1 + eta2)\n",
    "    theta2 = - 0.5  / (eta1*eta1 + eta2)\n",
    "    cost_et =  -(x*theta1 - theta2*x*x - 0.25*theta1*theta1/theta2 - 0.5*np.log(np.pi) +0.5*T.log(theta2))\n",
    "    params = [eta1, eta2]\n",
    "    ests, updates = update_method(cost_et, params, **kwargs)\n",
    "    train_et = theano.function(inputs=[x], updates=updates)\n",
    "    mu_list, sig_list = [], []\n",
    "    for xt in Xt:\n",
    "        train_et(xt)\n",
    "        if ests is not None:\n",
    "            mu_est, sigma2_est = et2lb(ests[0].get_value(), ests[1].get_value())\n",
    "        else:\n",
    "            mu_est, sigma2_est = et2lb(eta1.get_value(), eta2.get_value())\n",
    "        mu_list.append(mu_est)\n",
    "        sig_list.append(sigma2_est)\n",
    "    return [mu_list, sig_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem = Memory(cachedir='/tmp/joblib')\n",
    "#train_on_lb_cache = mem.cache(train_on_lb)\n",
    "#train_on_th_cache = mem.cache(train_on_th)\n",
    "#train_on_et_cache = mem.cache(train_on_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_updates(cost, params, alpha, known_grads=None):\n",
    "    if known_grads is None:\n",
    "        updates = [(param, param - alpha*T.grad(cost, param)) for param in params]\n",
    "    else:\n",
    "        updates = [(param, param - alpha*T.grad(cost, param, known_grads[param])) for param in params]\n",
    "    return None, updates\n",
    "\n",
    "def gradient_updates_2(cost, params, alpha):\n",
    "    t = theano.shared(1.)\n",
    "    updates= [(t, t+1)]\n",
    "    updates.extend([(param, param - np.power(t,-alpha)*T.grad(cost, param)) \n",
    "                    for param in params])\n",
    "    return None, updates\n",
    "\n",
    "def gradient_updates_momentum(cost, params, alpha, momentum):\n",
    "    \"\"\"\n",
    "    http://caffe.berkeleyvision.org/tutorial/solver.html\n",
    "    \"\"\"\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    updates = []\n",
    "    for param in params:\n",
    "        V = theano.shared(param.get_value()*0.)\n",
    "        updates.append((param, param + V))\n",
    "        updates.append((V, momentum*V - alpha * T.grad(cost, param)))\n",
    "    return None, updates\n",
    "\n",
    "# on peut rajouter d'autres méthodes ici\n",
    "\n",
    "def gradient_updates_average(cost, params, alpha, t0):\n",
    "    updates = []\n",
    "    ave_est = []\n",
    "    t = theano.shared(0.)\n",
    "    updates.append((t, t+1))   \n",
    "    for param in params:\n",
    "        ave = theano.shared(0.)\n",
    "        ave_est.append(ave)\n",
    "        updates.append((ave,\n",
    "                        T.switch((t+1) <= t0, param - alpha*T.grad(cost, param),\n",
    "                                 (t-t0)/(t-t0+1.) * ave +\n",
    "                                 (param - alpha*T.grad(cost, param)) / (t-t0+1.))\n",
    "                       ))\n",
    "        updates.append((param, param - alpha*T.grad(cost, param)))\n",
    "    return ave_est, updates\n",
    "\n",
    "def gradient_updates_adam(cost, params, alpha, beta1, beta2):\n",
    "    assert beta1 < 1 and beta1 >= 0\n",
    "    assert beta2 < 1 and beta2 >= 0\n",
    "    updates = []\n",
    "    t = theano.shared(1.)\n",
    "    updates.append((t, t+1))\n",
    "    for param in params:\n",
    "        gt = T.grad(cost, param) \n",
    "        mt = theano.shared(0.)\n",
    "        updates.append((mt, beta1 * mt + (1-beta1) * gt))\n",
    "        vt = theano.shared(0.)\n",
    "        updates.append((vt, beta2 * vt + (1-beta2) * gt * gt))\n",
    "        alpha_t = theano.shared(0.)\n",
    "        updates.append((alpha_t, \n",
    "                        alpha*T.sqrt(1 - beta2**t)/(1 - beta1**t)))\n",
    "        updates.append((param, param - alpha_t*mt/(T.sqrt(vt + 1e-8))))\n",
    "    return None, updates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this_result = train_on_lb(lb_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates, alpha=0.001)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_2, alpha=0.6)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_momentum, alpha=0.001,momentum=0.95)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_average, alpha=0.001,t0=500)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_average, alpha=0.001,t0=500)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_average, alpha=0.1,t0=500)\n",
    "#this_result = train_on_lb(lb_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "#this_result = train_on_th(th_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "#this_result = train_on_et(et_init[0], gradient_updates_adam, alpha=0.001, beta1=0.9, beta2=0.999)\n",
    "#print(this_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization SGD via Theano (source space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_params(init, *params):\n",
    "    return list(it.product(*params, init))\n",
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "alpha2s = [0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "momentums = [0.9, 0.95, 0.99]\n",
    "t0s = [10, 30, 100]\n",
    "beta1s = [0.9]\n",
    "beta2s = [0.95, 0.99, 0.999]\n",
    "params_SGD_lb = gen_params(lb_init, alphas)\n",
    "params_SGD2_lb = gen_params(lb_init, alpha2s)\n",
    "params_SGD_momentum_lb = gen_params(lb_init, alphas, momentums)\n",
    "params_SGD_ave_lb = gen_params(lb_init, alphas, t0s)\n",
    "params_SGD_adam_lb = gen_params(lb_init, alphas, beta1s, beta2s)\n",
    "\n",
    "params_SGD_th = gen_params(th_init, alphas)\n",
    "params_SGD2_th = gen_params(th_init, alpha2s)\n",
    "params_SGD_momentum_th = gen_params(th_init, alphas, momentums)\n",
    "params_SGD_ave_th = gen_params(th_init, alphas, t0s)\n",
    "params_SGD_adam_th = gen_params(th_init, alphas, beta1s, beta2s)\n",
    "\n",
    "params_SGD_et = gen_params(et_init, alphas)\n",
    "params_SGD2_et = gen_params(et_init, alpha2s)\n",
    "params_SGD_momentum_et = gen_params(et_init, alphas, momentums)\n",
    "params_SGD_ave_et = gen_params(et_init, alphas, t0s)\n",
    "params_SGD_adam_et = gen_params(et_init, alphas, beta1s, beta2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SGD_lb = Parallel(n_jobs=4)(delayed(train_on_lb_cache)(lb, \n",
    "                                                          gradient_updates, \n",
    "                                                          alpha=alpha)\n",
    "                               for alpha, lb in params_SGD_lb)\n",
    "\n",
    "SGD2_lb = Parallel(n_jobs=4)(delayed(train_on_lb_cache)(lb, \n",
    "                                                           gradient_updates_2, \n",
    "                                                           alpha=alpha)\n",
    "                               for alpha, lb in params_SGD2_lb)\n",
    "\n",
    "SGD_momentum_lb = Parallel(n_jobs=4)(delayed(train_on_lb_cache)(lb, \n",
    "                                                                   gradient_updates_momentum,\n",
    "                                                                   alpha=alpha,\n",
    "                                                                   momentum=momentum)\n",
    "                               for alpha, momentum, lb in params_SGD_momentum_lb)\n",
    "\n",
    "SGD_ave_lb = Parallel(n_jobs=4)(delayed(train_on_lb_cache)(lb, \n",
    "                                                              gradient_updates_average,\n",
    "                                                              alpha=alpha,\n",
    "                                                              t0=t0)\n",
    "                                   for alpha, t0, lb in params_SGD_ave_lb)\n",
    "\n",
    "SGD_adam_lb = Parallel(n_jobs=4)(delayed(train_on_lb_cache)(lb, \n",
    "                                                               gradient_updates_adam, \n",
    "                                                               alpha=alpha,\n",
    "                                                               beta1=beta1, \n",
    "                                                               beta2=beta2\n",
    "                                                              )\n",
    "                               for alpha, beta1, beta2, lb in params_SGD_adam_lb)\n",
    "\n",
    "resultats_lb = [SGD_lb, SGD2_lb, SGD_momentum_lb, SGD_ave_lb, SGD_adam_lb]\n",
    "dump(resultats_lb, 'resultats_lb.pklz', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SGD_th = Parallel(n_jobs=4)(delayed(train_on_th_cache)(th, \n",
    "                                                          gradient_updates, \n",
    "                                                          alpha=alpha)\n",
    "                               for alpha, th in params_SGD_th)\n",
    "\n",
    "SGD2_th = Parallel(n_jobs=4)(delayed(train_on_th_cache)(th, \n",
    "                                                           gradient_updates_2, \n",
    "                                                           alpha=alpha)\n",
    "                               for alpha, th in params_SGD2_th)\n",
    "\n",
    "SGD_momentum_th = Parallel(n_jobs=4)(delayed(train_on_th_cache)(th, \n",
    "                                                                   gradient_updates_momentum,\n",
    "                                                                   alpha=alpha,\n",
    "                                                                   momentum=momentum)\n",
    "                               for alpha, momentum, th in params_SGD_momentum_th)\n",
    "\n",
    "SGD_ave_th = Parallel(n_jobs=4)(delayed(train_on_th_cache)(th, \n",
    "                                                              gradient_updates_average,\n",
    "                                                              alpha=alpha,\n",
    "                                                              t0=t0)\n",
    "                                   for alpha, t0, th in params_SGD_ave_th)\n",
    "\n",
    "SGD_adam_th = Parallel(n_jobs=4)(delayed(train_on_th_cache)(th, \n",
    "                                                               gradient_updates_adam, \n",
    "                                                               alpha=alpha,\n",
    "                                                               beta1=beta1, \n",
    "                                                               beta2=beta2\n",
    "                                                              )\n",
    "                               for alpha, beta1, beta2, th in params_SGD_adam_th)\n",
    "\n",
    "resultats_th = [SGD_th, SGD2_th, SGD_momentum_th, SGD_ave_th, SGD_adam_th]\n",
    "dump(resultats_th, 'resultats_th.pklz', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SGD_et = Parallel(n_jobs=4)(delayed(train_on_et_cache)(et, \n",
    "                                                          gradient_updates, \n",
    "                                                          alpha=alpha)\n",
    "                               for alpha, et in params_SGD_et)\n",
    "\n",
    "SGD2_et = Parallel(n_jobs=4)(delayed(train_on_et_cache)(et, \n",
    "                                                           gradient_updates_2, \n",
    "                                                           alpha=alpha)\n",
    "                               for alpha, et in params_SGD2_et)\n",
    "\n",
    "SGD_momentum_et = Parallel(n_jobs=4)(delayed(train_on_et_cache)(et, \n",
    "                                                                   gradient_updates_momentum,\n",
    "                                                                   alpha=alpha,\n",
    "                                                                   momentum=momentum)\n",
    "                               for alpha, momentum, et in params_SGD_momentum_et)\n",
    "\n",
    "SGD_ave_et = Parallel(n_jobs=4)(delayed(train_on_et_cache)(et, \n",
    "                                                              gradient_updates_average,\n",
    "                                                              alpha=alpha,\n",
    "                                                              t0=t0)\n",
    "                                   for alpha, t0, et in params_SGD_ave_et)\n",
    "\n",
    "SGD_adam_et = Parallel(n_jobs=4)(delayed(train_on_et_cache)(et, \n",
    "                                                               gradient_updates_adam, \n",
    "                                                               alpha=alpha,\n",
    "                                                               beta1=beta1, \n",
    "                                                               beta2=beta2\n",
    "                                                              )\n",
    "                               for alpha, beta1, beta2, et in params_SGD_adam_et)\n",
    "\n",
    "resultats_et = [SGD_et, SGD2_et, SGD_momentum_et, SGD_ave_et, SGD_adam_et]\n",
    "dump(resultats_et, 'resultats_et.pklz', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultats_et = load('resultats_et.pklz')\n",
    "print(resultats_et.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "\n",
    "def customize_boxplot(ax, bp):\n",
    "    for box in bp['boxes']:\n",
    "        # change outline color\n",
    "        box.set( color='#7570b3', linewidth=2)\n",
    "        # change fill color\n",
    "        box.set( facecolor = '#1b9e77' )\n",
    "    ## change color and linewidth of the whiskers\n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color='#7570b3', linewidth=2)\n",
    "    ## change color and linewidth of the caps\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color='#7570b3', linewidth=2)\n",
    "    ## change color and linewidth of the medians\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='#b2df8a', linewidth=2)\n",
    "    ## change the style of fliers and their fill\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "    ## Custom x-axis labels\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    return ax, bp\n",
    "    \n",
    "def ll_boxplot(list_mu_sig_list, X, ticks):\n",
    "    data_to_plot = []\n",
    "    labels = [str(n) for n in ticks]\n",
    "    for n in ticks:\n",
    "        data_n = [ave_ll(mu_list[n-1], sig_list[n-1], X) \n",
    "                  for mu_list, sig_list in list_mu_sig_list]\n",
    "        data_to_plot.append(data_n)    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax, bp = customize_boxplot(ax, bp)\n",
    "    return fig\n",
    " \n",
    "def mu_sig_plot(list_mu_sig_list):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for mu_list, sig_list in list_mu_sig_list:\n",
    "        plt.plot(mu_list, color='blue')\n",
    "        plt.plot(sig_list, color='green')\n",
    "\n",
    "def kl_boxplot(list_mu_sig_list, ticks):\n",
    "    data_to_plot = []\n",
    "    labels = [str(n) for n in ticks]\n",
    "    for n in ticks:\n",
    "        data_n = [kl(mu_true, sigma2_true, mu_list[n-1], sig_list[n-1]) \n",
    "                  for mu_list, sig_list in list_mu_sig_list]\n",
    "        data_to_plot.append(data_n)    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    bp = ax.boxplot(data_to_plot, patch_artist=True)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax, bp = customize_boxplot(ax, bp)\n",
    "    return fig, ax\n",
    "\n",
    "def ll_kl_gboxplot(LL_KL_list, ticks):\n",
    "    def setBoxColors(bp):\n",
    "        colors = ['green', 'black']\n",
    "        for box, col in zip(bp['boxes'], colors):\n",
    "            box.set( color=col, linewidth=2)\n",
    "        for whisker, col in zip(bp['whiskers'], colors):\n",
    "            whisker.set(color=col, linewidth=2)    \n",
    "        for cap, col in zip(bp['caps'], colors):\n",
    "            cap.set(color=col, linewidth=2)\n",
    "        ## change color and linewidth of the medians\n",
    "        for median, col in zip(bp['medians'], colors):\n",
    "            median.set(color=col, linewidth=2)\n",
    "        ## change the style of fliers and their fill\n",
    "        for flier, col in zip(bp['fliers'], colors):\n",
    "            flier.set(marker='o', color=col, alpha=0.5)\n",
    "    labels = [str(n) for n in ticks]\n",
    "    ax = plt.gca()\n",
    "    plt.hold(True)\n",
    "    for t, tick in enumerate(ticks):\n",
    "        LL = [LL[t] for LL, KL in LL_KL_list]\n",
    "        KL = [KL[t] for LL, KL in LL_KL_list]\n",
    "        data_to_plot = [LL, KL]   \n",
    "        bp = ax.boxplot(data_to_plot, patch_artist=True, \n",
    "                        positions = [t*3+1, t*3+2], \n",
    "                        showfliers=False, \n",
    "                        widths = 0.6)\n",
    "        setBoxColors(bp)\n",
    "    ax.set_xlim([0,3*len(ticks)+1])\n",
    "    ax.set_xticklabels([str(tick) for tick in ticks])\n",
    "    ax.set_xticks([t*3+1.5 for t in range(len(ticks))])\n",
    "    # draw temporary red and blue lines and use them to create a legend\n",
    "    hG, = plt.plot([], color='green', label='valid')\n",
    "    hK, = plt.plot([], color='black', label='KL')\n",
    "    plt.legend() #(hG, hK),('valid', 'train'))\n",
    "    hG.set_visible(False)\n",
    "    hK.set_visible(False) \n",
    "\n",
    "def setBoxColors(bp, colors):\n",
    "    for box, whisker, cap ,median, flier, col in zip(bp['boxes'], bp['whiskers'], bp['caps'], \n",
    "                                                     bp['medians'], bp['fliers'], colors):\n",
    "            box.set(color=col, linewidth=2)\n",
    "            whisker.set(color=col, linewidth=2)    \n",
    "            cap.set(color=col, linewidth=2)\n",
    "            median.set(color=col, linewidth=2)\n",
    "            flier.set(marker='o', color=col, alpha=0.5)\n",
    "    return bp\n",
    "    \n",
    "def gboxplot(ticks, lists, lists_labels):\n",
    "    ticks_labels = [str(__) for __ in ticks]\n",
    "    n = len(lists)\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n))\n",
    "    ax = plt.gca()\n",
    "    plt.hold(True)\n",
    "    for t, tick in enumerate(ticks):\n",
    "        data_to_plot = [[l[t] for l in ll if not np.any(np.isnan(l[t]))] for ll in lists]   \n",
    "        bp = ax.boxplot(data_to_plot, patch_artist=True, \n",
    "                        positions = range(t*(n+1)+1,(t+1)*(n+1)),\n",
    "                        showfliers=True, #False, \n",
    "                        widths = 0.6)\n",
    "        bp = setBoxColors(bp, colors)\n",
    "    ax.set_xlim([0, len(ticks)*(n+1)+1])\n",
    "    ax.set_xticklabels([str(tick) for tick in ticks])\n",
    "    ax.set_xticks([(2*t+1)*(n+1)/2 for t in range(len(ticks))])\n",
    "    # draw temporary red and blue lines and use them to create a legend\n",
    "    for color, label in zip(colors, lists_labels):\n",
    "        h, = plt.plot([], color=color, label=label)\n",
    "        #h.set_visible(False)\n",
    "    plt.legend(loc=4) #(hG, hK),('valid', 'train')) \n",
    "\n",
    "def disp_results_all(results):\n",
    "    n = len(results)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n))\n",
    "    table = ListTable()\n",
    "    table.append(['', 'Train', 'Test', 'mu', 'sigma^2'])\n",
    "    table.append(['True', ave_ll(mu_true, sigma2_true, Xt), \n",
    "                  ave_ll(mu_true, sigma2_true, Xtest),\n",
    "                  str(mu_true), str(sigma_true**2)])\n",
    "    for c, result in enumerate(results):\n",
    "        train, test, mu, sig, method = result\n",
    "        axes[0,0].plot(train, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,0].set_title('Average likelihood on train set')\n",
    "        axes[0,0].legend(loc=4)\n",
    "        axes[0,1].plot(test, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[0,1].set_title('Average likelihood on test set')\n",
    "        axes[0,1].legend(loc=4)\n",
    "        axes[1,0].plot(mu, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,0].set_title('Estimates of $\\mu$')\n",
    "        axes[1,0].legend(loc=4)\n",
    "        axes[1,1].plot(sig, color=colors[c], linewidth=1, linestyle='-', label=method)\n",
    "        axes[1,1].set_title('Estimates of $\\sigma^2$')\n",
    "        axes[1,1].legend(loc=4)\n",
    "        table.append([method, train[-1], test[-1],mu[-1], sig[-1]])\n",
    "    display(HTML(table._repr_html_()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Figure 1 : Formule exacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultats_exact = load('resultats_exact.pklz')\n",
    "mu_list, sig_list, ave_ll_train, ave_ll_test, KL_exact = resultats_exact\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(np.arange(1,len(ave_ll_train)+1), mu_list, label=\"$\\hat\\mu$\")\n",
    "plt.plot(np.arange(1,len(ave_ll_train)+1), sig_list, label=\"$\\hat\\sigma^{2}$\")\n",
    "ax.set_xlabel('N')\n",
    "ax.set_ylim([0.8,4.2])\n",
    "plt.legend(loc=7)\n",
    "plt.savefig('exact_est.png', bbox_inches='tight', dpi=300)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.arange(1,len(ave_ll_train)+1), ave_ll_train, label='train')\n",
    "ax1.plot(np.arange(1,len(ave_ll_train)+1), ave_ll_test, label='valid')\n",
    "ax1.set_xlabel('N')\n",
    "ax1.set_ylim([-2.2,-2.1])\n",
    "# Make the y-axis label and tick labels match the line color.\n",
    "ax1.set_ylabel('average log likelihood')\n",
    "plt.legend(loc=6)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(1,len(ave_ll_train)+1), KL_exact, label=\"KL\", color='k')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('k')\n",
    "ax2.set_ylim([-0.001,0.04])\n",
    "ax2.set_ylabel(\"Kullback-Leibler divergence(true || est)\")\n",
    "plt.legend(loc=7)\n",
    "plt.savefig('exact_llkl.png', bbox_inches='tight', dpi=300)\n",
    "#plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 : Graphiques SGD\n",
    "\n",
    "même init alpha différents valeurs de sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticks = [200, 2000, 20000]\n",
    "alphas = np.round(np.logspace(-1, -3, 5), 4)\n",
    "SGD_lb0_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb_cache) (lb_init[0], gradient_updates, alpha=alpha) \n",
    "                                               for alpha in alphas)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(alphas)))\n",
    "plt.figure()\n",
    "for (mu_list, sig_list), col, alpha in zip(SGD_lb0_alpha, colors, alphas):\n",
    "        plt.plot(range(20000), mu_list, color=col, linewidth=1, linestyle='-', label=str(alpha))\n",
    "        plt.plot(range(20000), sig_list, color=col, linewidth=1, linestyle='-')\n",
    "plt.legend(loc=7)\n",
    "plt.title('$\\mu$ and $\\sigma^2$ estimates with SGD($\\Lambda$) and same init.')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 28000])\n",
    "ax.set_xticks([0, 5000, 10000, 15000, 20000])\n",
    "plt.savefig('sgd_lb0_est.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "alphas2 = np.round(np.logspace(-4, -6, 5), 6)\n",
    "SGD_th0_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th_cache) (th_init[0], gradient_updates, alpha=alpha) \n",
    "                                               for alpha in alphas2)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(alphas)))\n",
    "plt.figure()\n",
    "for (mu_list, sig_list), col, alpha in zip(SGD_th0_alpha, colors, alphas2):\n",
    "        plt.plot(range(20000), mu_list, color=col, linewidth=1, linestyle='-', label=str(alpha))\n",
    "        plt.plot(range(20000), sig_list, color=col, linewidth=1, linestyle='-')\n",
    "plt.legend(loc=7)\n",
    "plt.title('$\\mu$ and $\\sigma^2$ estimates with SGD($\\Theta$) and same init.')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 28000])\n",
    "ax.set_xticks([0, 5000, 10000, 15000, 20000])\n",
    "plt.savefig('sgd_th0_est.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "SGD_et0_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et_cache) (et_init[0], gradient_updates, alpha=alpha) \n",
    "                                               for alpha in alphas)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(alphas)))\n",
    "plt.figure()\n",
    "for (mu_list, sig_list), col, alpha in zip(SGD_et0_alpha, colors, alphas):\n",
    "        plt.plot(range(20000), mu_list, color=col, linewidth=1, linestyle='-', label=str(alpha))\n",
    "        plt.plot(range(20000), sig_list, color=col, linewidth=1, linestyle='-')\n",
    "plt.legend(loc=7)\n",
    "plt.title('$\\mu$ and $\\sigma^2$ estimates with SGD($H$) and same init.')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 28000])\n",
    "ax.set_xticks([0, 5000, 10000, 15000, 20000])\n",
    "plt.savefig('sgd_et0_est.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "#ll_boxplot(SGD_lb0_alpha, Xt, ticks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 : LL sur les 3 espaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUQ8piQvIuIxJXkREY/9f1xcukf5kecfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e43c320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113162780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticks = [200, 2000, 5000, 10000, 20000]\n",
    "alpha_SGD_lb = 0.9 # 0.0316\n",
    "alpha_SGD_th = 0.9 # 3.2e-5\n",
    "alpha_SGD_et = alpha_SGD_lb\n",
    "\n",
    "update = gradient_updates_2\n",
    "\n",
    "mem.clear()\n",
    "#@mem.cache\n",
    "def lb_fun(lb, fun):\n",
    "    mu_list, sig_list = train_on_lb(lb, update, alpha=alpha_SGD_lb)\n",
    "    return [fun(mu_list[tick-1], sig_list[tick-1]) for tick in ticks]\n",
    "\n",
    "#@mem.cache\n",
    "def th_fun(th, fun):\n",
    "    mu_list, sig_list = train_on_th(th, update, alpha=alpha_SGD_th)\n",
    "    return [fun(mu_list[tick-1], sig_list[tick-1]) for tick in ticks]\n",
    "\n",
    "#@mem.cache\n",
    "def et_fun(et, fun):\n",
    "    mu_list, sig_list = train_on_et(et, update, alpha=alpha_SGD_et)\n",
    "    return [fun(mu_list[tick-1], sig_list[tick-1]) for tick in ticks]\n",
    "\n",
    "def kl_fun(m,s):\n",
    "    return kl(mu_true, sigma2_true, m, s)\n",
    "def ll_fun(m,s):\n",
    "    return ave_ll(m, s, Xtest)\n",
    "\n",
    "SGD_lb_alpha_ll = Parallel(n_jobs=-1, verbose=0)(delayed(lb_fun)(lb, ll_fun) for lb in lb_init)\n",
    "SGD_th_alpha_ll = Parallel(n_jobs=-1, verbose=0)(delayed(th_fun)(th, ll_fun) for th in th_init)\n",
    "SGD_et_alpha_ll = Parallel(n_jobs=-1, verbose=0)(delayed(et_fun)(et, ll_fun) for et in et_init)\n",
    "gboxplot(ticks, [SGD_lb_alpha_ll, SGD_et_alpha_ll] , ['lb', 'et'])\n",
    "plt.figure()\n",
    "gboxplot(ticks, [SGD_lb_alpha_ll, SGD_th_alpha_ll, SGD_et_alpha_ll] , ['lb', 'th', 'et'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = [200, 2000, 5000, 10000, 20000]\n",
    "alpha_SGD_lb = 0.0316\n",
    "alpha_SGD_th = 3.2e-5\n",
    "alpha_SGD_et = alpha_SGD_lb\n",
    "\n",
    "def lb_ll_kl(lb):\n",
    "    mu_list, sig_list = train_on_lb(lb, gradient_updates, alpha=alpha_SGD_lb)\n",
    "    LL = Parallel(n_jobs=-1, verbose=0)(delayed(ave_ll)(mu_list[tick-1], sig_list[tick-1], Xtest) \n",
    "                                                 for tick in ticks)\n",
    "    KL = Parallel(n_jobs=-1, verbose=0)(delayed(kl) (mu_true, sigma2_true, mu_list[tick-1], sig_list[tick-1]) \n",
    "                                                 for tick in ticks)\n",
    "    return LL, KL\n",
    "\n",
    "def th_ll_kl(th):\n",
    "    mu_list, sig_list = train_on_th(th, gradient_updates, alpha=alpha_SGD_th)\n",
    "    LL = Parallel(n_jobs=-1, verbose=0)(delayed(ave_ll)(mu_list[tick-1], sig_list[tick-1], Xtest) \n",
    "                                                 for tick in ticks)\n",
    "    KL = Parallel(n_jobs=-1, verbose=0)(delayed(kl) (mu_true, sigma2_true, mu_list[tick-1], sig_list[tick-1]) \n",
    "                                                 for tick in ticks)\n",
    "    return LL, KL\n",
    "\n",
    "def et_ll_kl(et):\n",
    "    mu_list, sig_list = train_on_et(et, gradient_updates, alpha=alpha_SGD_et)\n",
    "    LL = Parallel(n_jobs=-1, verbose=0)(delayed(ave_ll)(mu_list[tick-1], sig_list[tick-1], Xtest) \n",
    "                                                 for tick in ticks)\n",
    "    KL = Parallel(n_jobs=-1, verbose=0)(delayed(kl) (mu_true, sigma2_true, mu_list[tick-1], sig_list[tick-1]) \n",
    "                                                 for tick in ticks)\n",
    "    return LL, KL\n",
    "\n",
    "lb_ll_kl_cache = mem.cache(lb_ll_kl)\n",
    "th_ll_kl_cache = mem.cache(th_ll_kl)\n",
    "et_ll_kl_cache = mem.cache(et_ll_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem.clear()\n",
    "SGD_lb_alpha = [lb_ll_kl_cache(lb) for lb in lb_init]\n",
    "SGD_th_alpha = [th_ll_kl_cache(th) for th in th_init]\n",
    "SGD_et_alpha = [et_ll_kl_cache(et) for et in et_init]\n",
    "\n",
    "plt.figure()\n",
    "ll_kl_gboxplot(SGD_lb_alpha, ticks)\n",
    "plt.savefig('sgd_lb_llkl_bp.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.figure()\n",
    "ll_kl_gboxplot(SGD_th_alpha, ticks)\n",
    "plt.savefig('sgd_th_llkl_bp.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.figure()\n",
    "ll_kl_gboxplot(SGD_et_alpha, ticks)\n",
    "plt.savefig('sgd_et_llkl_bp.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "#SGD_lb_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb_cache)\n",
    "#                                                  (lb, gradient_updates, alpha=alpha_SGD_lb) for lb in lb_init)\n",
    "#SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th_cache)\n",
    "#                                                  (th, gradient_updates, alpha=alpha_SGD_th) for th in th_init)\n",
    "#SGD_et_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et_cache)\n",
    "#                                                  (et, gradient_updates, alpha=alpha_SGD_et) for et in et_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticks = [200, 2000, 5000, 10000, 20000]\n",
    "alpha_SGD_lb = 0.0316\n",
    "alpha_SGD_th = 3.2e-5\n",
    "alpha_SGD_et = alpha_SGD_lb\n",
    "SGD_lb_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb_cache)\n",
    "                                                  (lb, gradient_updates, alpha=alpha_SGD_lb) for lb in lb_init)\n",
    "SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th_cache)\n",
    "                                                  (th, gradient_updates, alpha=alpha_SGD_th) for th in th_init)\n",
    "SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et_cache)\n",
    "                                                  (et, gradient_updates, alpha=alpha_SGD_et) for et in et_init)\n",
    "for alpha in alphas:\n",
    "    print ('Alpha = '+str(alpha))\n",
    "    SGD_lb_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb)\n",
    "                                                  (lb, gradient_updates, alpha=alpha) for lb in lb_init)\n",
    "    #kl_boxplot(SGD_lb_alpha, ticks)\n",
    "    mu_sig_plot(SGD_lb_alpha)\n",
    "    SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th)\n",
    "                                                  (th, gradient_updates, alpha=alpha) for th in th_init)\n",
    "    mu_sig_plot(SGD_th_alpha)\n",
    "    #kl_boxplot(SGD_th_alpha, ticks)\n",
    "    SGD_et_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et)\n",
    "                                                  (et, gradient_updates, alpha=alpha) for et in et_init)\n",
    "    mu_sig_plot(SGD_et_alpha)\n",
    "    #kl_boxplot(SGD_et_alpha, ticks)\n",
    "    \n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha in alpha2s:\n",
    "    print ('Alpha = '+str(alpha))\n",
    "    SGD_lb_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb)\n",
    "                                                  (lb, gradient_updates_2, alpha=alpha) for lb in lb_init)\n",
    "    #kl_boxplot(SGD_lb_alpha, ticks)\n",
    "    mu_sig_plot(SGD_lb_alpha)\n",
    "    SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th)\n",
    "                                                  (th, gradient_updates_2, alpha=alpha) for th in th_init)\n",
    "    mu_sig_plot(SGD_th_alpha)\n",
    "    #kl_boxplot(SGD_th_alpha, ticks)\n",
    "    SGD_et_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et)\n",
    "                                                  (et, gradient_updates_2, alpha=alpha) for et in et_init)\n",
    "    mu_sig_plot(SGD_et_alpha)\n",
    "    #kl_boxplot(SGD_et_alpha, ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha, momentum in it.product(alphas, momentums):\n",
    "    print ('Alpha = '+str(alpha)+ '    Momentum '+ str(momentum))\n",
    "    SGD_lb_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_lb)\n",
    "                                                  (lb, gradient_updates_momentum, alpha=alpha, momentum=momentum)\n",
    "                                                  for lb in lb_init)\n",
    "    #kl_boxplot(SGD_lb_alpha, ticks)\n",
    "    mu_sig_plot(SGD_lb_alpha)\n",
    "    SGD_th_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_th)\n",
    "                                                  (th,  gradient_updates_momentum, alpha=alpha, momentum=momentum)\n",
    "                                                  for th in th_init)\n",
    "    mu_sig_plot(SGD_th_alpha)\n",
    "    #kl_boxplot(SGD_th_alpha, ticks)\n",
    "    SGD_et_alpha = Parallel(n_jobs=-1, verbose=0)(delayed(train_on_et)\n",
    "                                                  (et,  gradient_updates_momentum, alpha=alpha, momentum=momentum)\n",
    "                                                  for et in et_init)\n",
    "    mu_sig_plot(SGD_et_alpha)\n",
    "    #kl_boxplot(SGD_et_alpha, ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### results_0005 = [train_on_lb_cache(lb, gradient_updates, alpha=0.005) for lb in lb_init]\n",
    "ll_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#ll_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#results_001 = [train_on_lb_cache(lb, gradient_updates, alpha=0.01) for lb in lb_init]\n",
    "#ll_boxplot(results_001, Xt, [200, 2000, 20000])\n",
    "kl_boxplot(results_0005, Xt, [200, 2000, 20000])\n",
    "#ll_boxplot(results_001, Xt, [200, 2000, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.round(np.linspace(1e-4, 1e-6, 5), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
